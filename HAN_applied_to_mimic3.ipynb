{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/my_project/blob/main/HAN_applied_to_mimic3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut06y6gCZETG"
      },
      "outputs": [],
      "source": [
        "# 참고 논문 : Description-based Label Attention Classifier for Explainable ICD-9 Classification\n",
        "# https://git.io/JzOyk\n",
        "# https://cpm0722.github.io/pytorch-implementation/transformer \n",
        "# https://hugrypiggykim.com/2018/12/09/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQBma8NqZHaX",
        "outputId": "fdc49dd6-904a-4ac3-8b5b-07201379f587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.107.38.10:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.107.38.10:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.tpu.topology.Topology at 0x7f197ee45e90>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSYR_v9JZHyz"
      },
      "outputs": [],
      "source": [
        "# 해당 모델을 fit 하게되면 TPU로 학습하게됨\n",
        "#with strategy.scope():\n",
        "#  model = create_model()\n",
        "#  model.compile(optimizer='adam',\n",
        "#                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#                metrics=['sparse_categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JtxV--5cg3i"
      },
      "source": [
        "# Settings "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MOPeGuHbiYe",
        "outputId": "5139926d-7eee-4af5-e116-957dcb0e83d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# 구글드라이브에 핸들링한 데이터 넣어둠\n",
        "# .(마침표) 남겨둔 데이터 FOR sentence 나누기\n",
        "# 파일명 : notes_labeled_binarized / notes_labeled_50_binarized\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EtpQU4RcI2z",
        "outputId": "34d8ee6e-472a-4ba3-cddf-e252303ef9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "#from keras.engine.topology import Layer 수정해야함\n",
        "#from tensorflow.python.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from keras.layers import Input, Embedding, Dense\n",
        "from keras.layers import Lambda, Permute, RepeatVector, Multiply\n",
        "from keras.layers import Bidirectional, TimeDistributed\n",
        "from keras.layers import CuDNNGRU, GRU\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer  # 토큰화하는 패키지\n",
        "from keras.preprocessing.sequence import pad_sequences # padding 을 위한 패키지\n",
        "from tensorflow.keras.utils import to_categorical #from keras.utils import to_categorical # 범주형 변수로 만들어줌 / 실행안되는 이유확인\n",
        "from nltk.tokenize import sent_tokenize  # 문장을 token화 시킴\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "import gc\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcalbTGned7K"
      },
      "source": [
        "# Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xsNbJxC8cI5R"
      },
      "outputs": [],
      "source": [
        "final_data = pd.read_csv('gdrive/MyDrive/dataset/mimic data/final_data.csv')  #  encoding = 'c==p949' 생략"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# final_data의 text에서 각 document마다 period가 몇번 등장하는지 확인해보고자 함. period를 사용하지 않은 데이터도 있지 않을까 의심되어 확인해봄\n",
        "all_text = final_data['TEXT']"
      ],
      "metadata": {
        "id": "ce2e_5mUsk1C"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myString = all_text[1]\n",
        "print(myString.count('.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W20iGPTtEc-",
        "outputId": "39c2df06-abcc-48d2-f062-6b7bf3ea38fc"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(all_text))\n",
        "print(type(all_text))\n",
        "all_text_df = pd.DataFrame(all_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHnRuRAOtHB9",
        "outputId": "f52bb4d5-239f-472e-db6c-34b227da2b91"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12808\n",
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text[1].count('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVp3Ldm3t2gq",
        "outputId": "ce7268b2-972e-4ef3-890a-8d5c1381ae44"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(all_text)):\n",
        "  all_text_df['number_of_period'][i] = all_text[i].count('.')   # 12808"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsT86Q2ysw-z",
        "outputId": "e2f746a8-07f6-48ae-ca01-fad8cf49b872"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "uRddDl08tcxD",
        "outputId": "ca009ef1-be6f-4e82-d38e-914773eef2ab"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                TEXT  number_of_period\n",
              "0  admission date date of birth service cardiotho...                58\n",
              "1  admission date date of birth service history o...               120\n",
              "2  admission date date of birth service cardiotho...                72"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fa15aaa-2982-4be5-8df6-0b08023e33f5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>number_of_period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>admission date date of birth service cardiotho...</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>admission date date of birth service history o...</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>admission date date of birth service cardiotho...</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fa15aaa-2982-4be5-8df6-0b08023e33f5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6fa15aaa-2982-4be5-8df6-0b08023e33f5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6fa15aaa-2982-4be5-8df6-0b08023e33f5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UImVu6IbuTj2",
        "outputId": "9b869ee2-5872-4d25-dc30-7bd95f4e9990"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12808, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min(all_text_df['number_of_period'])  # period가 없는 문장이 존재함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjRr0mNduXQF",
        "outputId": "6120c732-1422-4578-b529-ef756084ed4d"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_text_df[all_text_df['number_of_period']<5]  # 72개의 document에서 period가 발견되지 않음 # 해당 데이터는 제거하고 실행해야할것으로 보임 / period가 거의 안사용되는 문장들도 존재함을 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Y3IikmfLuh2R",
        "outputId": "c031c3c6-1c58-41a9-a13a-bde44b2ca601"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    TEXT  number_of_period\n",
              "80     name admission date date of birth service adde...                 3\n",
              "136    name admission date date of birth service adde...                 4\n",
              "138    y name admission date date of birth service di...                 2\n",
              "139    name admission date date of birth service the ...                 3\n",
              "159    name admission date date of birth service card...                 4\n",
              "...                                                  ...               ...\n",
              "12696  name admission date date of birth service neur...                 1\n",
              "12715  name admission date date of birth service card...                 1\n",
              "12752  name admission date date of birth service surg...                 2\n",
              "12759  name admission date date of birth service orth...                 2\n",
              "12763  name admission date date of birth service medi...                 3\n",
              "\n",
              "[477 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a84d6159-2353-4bec-b6d4-9fcba9be94ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TEXT</th>\n",
              "      <th>number_of_period</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>name admission date date of birth service adde...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>name admission date date of birth service adde...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>y name admission date date of birth service di...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>name admission date date of birth service the ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>name admission date date of birth service card...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12696</th>\n",
              "      <td>name admission date date of birth service neur...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12715</th>\n",
              "      <td>name admission date date of birth service card...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12752</th>\n",
              "      <td>name admission date date of birth service surg...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12759</th>\n",
              "      <td>name admission date date of birth service orth...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12763</th>\n",
              "      <td>name admission date date of birth service medi...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>477 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a84d6159-2353-4bec-b6d4-9fcba9be94ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a84d6159-2353-4bec-b6d4-9fcba9be94ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a84d6159-2353-4bec-b6d4-9fcba9be94ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(all_text_df['number_of_period'],  bins = 200, color = \"blue\") "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "N70NcuU5urk5",
        "outputId": "42eb4c7c-0e5a-4f6e-d524-0914e90d6b99"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f18eb7d6e10>"
            ]
          },
          "metadata": {},
          "execution_count": 152
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU1khdAolFAL",
        "outputId": "94d9b731-1051-4366-8121-394c6053e27c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS', '038.9',\n",
              "       '244.9', '250.00', '272.0', '272.4', '276.1', '276.2', '285.1', '285.9',\n",
              "       '287.5', '305.1', '311', '33.24', '36.15', '37.22', '37.23', '38.91',\n",
              "       '38.93', '39.61', '39.95', '401.9', '403.90', '410.71', '412', '414.01',\n",
              "       '424.0', '427.31', '428.0', '45.13', '486', '496', '507.0', '511.9',\n",
              "       '518.81', '530.81', '584.9', '585.9', '599.0', '88.56', '88.72',\n",
              "       '96.04', '96.6', '96.71', '96.72', '99.04', '99.15', '995.92', 'V15.82',\n",
              "       'V45.81', 'V58.61'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "final_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NHH_Fw8VdUOv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# long sentence 할때 이 부분 숫자를 조정해야할듯\n",
        "# 데이터에서 max(sentence) 값을 적용해야할듯\n",
        "\n",
        "MAX_SENTENCES = 300 # 10 : 몇개의 문장\n",
        "MAX_SENTENCE_LENGTH = 50 # 25 :문장마다의 길이\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ_0AH1udZYa",
        "outputId": "80a286bb-1941-407d-f521-c8786d01edf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_x_data): 8000\n",
            "len(test_x_data): 4808\n"
          ]
        }
      ],
      "source": [
        "# train / test\n",
        "# y값 : top50의 하나씩 대입시킴\n",
        "train_x_data = final_data['TEXT'][:8000]\n",
        "train_y_data = final_data['244.9'][:8000]  \n",
        "test_x_data = final_data['TEXT'][8000:]\n",
        "test_y_data = final_data['244.9'][8000:]  \n",
        "\n",
        "print(\"len(train_x_data): {}\".format(len(train_x_data))) # 10000 / 8000\n",
        "print(\"len(test_x_data): {}\".format(len(test_x_data))) # 2808 / 4808\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZS-jbYykomX",
        "outputId": "8f91878f-8d2f-4058-8118-593e26e2a576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(train_x_data): 8000\n",
            "len(test_x_data): 4808\n",
            "len(train_y_data): 8000\n",
            "len(test_y_data): 4808\n"
          ]
        }
      ],
      "source": [
        "# train / test\n",
        "train_x_data = final_data['TEXT'][:8000]\n",
        "train_y_data = final_data[['038.9','244.9', '250.00', '272.0', '272.4', '276.1', '276.2', '285.1', '285.9', '287.5', '305.1',\n",
        "                           '311', '33.24', '36.15', '37.22', '37.23', '38.91','38.93', '39.61', '39.95', '401.9', '403.90',\n",
        "                           '410.71', '412', '414.01','424.0', '427.31', '428.0', '45.13', '486', '496', '507.0', '511.9',\n",
        "                           '518.81', '530.81', '584.9', '585.9', '599.0', '88.56', '88.72', '96.04', '96.6', '96.71', '96.72',\n",
        "                           '99.04', '99.15', '995.92', 'V15.82', 'V45.81', 'V58.61']][:8000]\n",
        "test_x_data = final_data['TEXT'][8000:]\n",
        "test_y_data = final_data[['038.9','244.9', '250.00', '272.0', '272.4', '276.1', '276.2', '285.1', '285.9', '287.5', '305.1', '311', '33.24', '36.15', '37.22', '37.23', '38.91','38.93', '39.61', '39.95', '401.9', '403.90', '410.71', '412', '414.01','424.0', '427.31', '428.0', '45.13', '486', '496', '507.0', '511.9', '518.81', '530.81', '584.9', '585.9', '599.0', '88.56', '88.72', '96.04', '96.6', '96.71', '96.72', '99.04', '99.15', '995.92', 'V15.82', 'V45.81', 'V58.61']][8000:]\n",
        "\n",
        "print(\"len(train_x_data): {}\".format(len(train_x_data))) # 10000 / 8000\n",
        "print(\"len(test_x_data): {}\".format(len(test_x_data))) # \n",
        "print(\"len(train_y_data): {}\".format(len(train_y_data))) # \n",
        "print(\"len(test_y_data): {}\".format(len(test_y_data))) #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOSyKZ8jdZdw",
        "outputId": "9298529c-5421-45b8-c208-434ad79ca339"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63993"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_x_data)\n",
        "tokenizer.fit_on_texts(test_x_data)\n",
        "len(tokenizer.word_index) # 63993\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpcB73z2dx0j",
        "outputId": "7b036cfa-5724-4be3-e9d1-ea654966ec19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "63994"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_nb_words = len(tokenizer.word_index) + 1  # 토큰나이즈했을때에 나타나는 단어개수만큼\n",
        "max_nb_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "oFO5Io7md_yD",
        "outputId": "cba9a64b-61d0-481a-b116-d6fac11bd6d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"admission date date of birth service cardiothoracic allergies bactrim ampicillin remeron attending chief complaint chest pressure cardiac tamponade cardiogenic shock major surgical or invasive procedure emergent sternotomy for pericardial window history of present illness underwent min. inv. pfo closure in on and chest pain for several days. did not resolve with pain med and had increasing sob. admitted to er for emergent eval. and bedside tte. started on dopamine drip for hypotension. past medical history s p min. inv. closure of patent foramen ovale stroke tia depression anxiety borderline hyperlipidemia herniation of cervical discs patella femoral syndrome s p bunionectomies social history denies tobacco. admits to occasional etoh. she is an employee of the children. she denies ivda and recreational drugs. family history father underwent cabg at age . cousin died of an mi at age . physical exam pt. in distress sbp 's lungs cta tachycardic rr no murmur or rubs palpable pedal pulses pertinent results mcv mch . mchc . rdw . plt ct baso . k . cl hco angap brief hospital course admitted through er as above and referred to ct emergent pericardial window pericardectomy via sternotomy as the patient was hypotensive.this was performed by dr. on and propofol drips. extubated and awoke neurologically intact. beta blockade started on pod and transferred out to the floor to start increasing her activity level. mediastinal tubes removed on pod . crepitus was noted on anterior chest wall after pleural tubes removed on pod . beta blockade also titrated up. crepitus improved and cxr confirmed. she made good progress and was discharged to home with vna services on pod . medications on admission asa mg daily discharge medications . docusate sodium mg capsule sig one capsule po bid times a day for months. disp capsule s refills . ranitidine hcl mg tablet sig one tablet po bid times a day for months. disp tablet s refills . aspirin mg tablet delayed release e.c. sig one tablet delayed release e.c. po daily daily . disp tablet delayed release e.c. s refills . acetaminophen mg tablet sig two tablet po q h every hours as needed. disp tablet s refills . oxycodone acetaminophen mg tablet sig tablets po q h every hours as needed for pain. disp tablet s refills . furosemide mg tablet sig one tablet po once a day for days. disp tablet s refills . potassium chloride meq packet sig one packet po once a day for days. disp packet s refills . metoprolol tartrate mg tablet sig . tablets po bid times a day . disp tablet s refills discharge disposition home with service facility discharge diagnosis s p emergent pericardial window via sternotomy cardiogenic shock tamponade s p min inv. pfo closure s p cva anxiety depression cervical disc herniation patella femoral syndrome borderline hyperlipidemia discharge condition stable discharge instructions may shower over incision and gently pat dry no lotions creams or powders on incision no driving for one month no lifting greater than pounds for weeks call for fever greater than redness or drainage followup instructions follow up with dr. follow up with dr. follow up with dr. completed by\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_x_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbURe44wd1tj",
        "outputId": "f54e2f5d-c10f-4ff7-a92a-1d092d303181"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['admission date date of birth service cardiothoracic allergies bactrim ampicillin remeron attending chief complaint chest pressure cardiac tamponade cardiogenic shock major surgical or invasive procedure emergent sternotomy for pericardial window history of present illness underwent min.',\n",
              " 'inv.',\n",
              " 'pfo closure in on and chest pain for several days.',\n",
              " 'did not resolve with pain med and had increasing sob.',\n",
              " 'admitted to er for emergent eval.',\n",
              " 'and bedside tte.',\n",
              " 'started on dopamine drip for hypotension.',\n",
              " 'past medical history s p min.',\n",
              " 'inv.',\n",
              " 'closure of patent foramen ovale stroke tia depression anxiety borderline hyperlipidemia herniation of cervical discs patella femoral syndrome s p bunionectomies social history denies tobacco.',\n",
              " 'admits to occasional etoh.',\n",
              " 'she is an employee of the children.',\n",
              " 'she denies ivda and recreational drugs.',\n",
              " 'family history father underwent cabg at age .',\n",
              " 'cousin died of an mi at age .',\n",
              " 'physical exam pt.',\n",
              " \"in distress sbp 's lungs cta tachycardic rr no murmur or rubs palpable pedal pulses pertinent results mcv mch .\",\n",
              " 'mchc .',\n",
              " 'rdw .',\n",
              " 'plt ct baso .',\n",
              " 'k .',\n",
              " 'cl hco angap brief hospital course admitted through er as above and referred to ct emergent pericardial window pericardectomy via sternotomy as the patient was hypotensive.this was performed by dr. on and propofol drips.',\n",
              " 'extubated and awoke neurologically intact.',\n",
              " 'beta blockade started on pod and transferred out to the floor to start increasing her activity level.',\n",
              " 'mediastinal tubes removed on pod .',\n",
              " 'crepitus was noted on anterior chest wall after pleural tubes removed on pod .',\n",
              " 'beta blockade also titrated up.',\n",
              " 'crepitus improved and cxr confirmed.',\n",
              " 'she made good progress and was discharged to home with vna services on pod .',\n",
              " 'medications on admission asa mg daily discharge medications .',\n",
              " 'docusate sodium mg capsule sig one capsule po bid times a day for months.',\n",
              " 'disp capsule s refills .',\n",
              " 'ranitidine hcl mg tablet sig one tablet po bid times a day for months.',\n",
              " 'disp tablet s refills .',\n",
              " 'aspirin mg tablet delayed release e.c.',\n",
              " 'sig one tablet delayed release e.c.',\n",
              " 'po daily daily .',\n",
              " 'disp tablet delayed release e.c.',\n",
              " 's refills .',\n",
              " 'acetaminophen mg tablet sig two tablet po q h every hours as needed.',\n",
              " 'disp tablet s refills .',\n",
              " 'oxycodone acetaminophen mg tablet sig tablets po q h every hours as needed for pain.',\n",
              " 'disp tablet s refills .',\n",
              " 'furosemide mg tablet sig one tablet po once a day for days.',\n",
              " 'disp tablet s refills .',\n",
              " 'potassium chloride meq packet sig one packet po once a day for days.',\n",
              " 'disp packet s refills .',\n",
              " 'metoprolol tartrate mg tablet sig .',\n",
              " 'tablets po bid times a day .',\n",
              " 'disp tablet s refills discharge disposition home with service facility discharge diagnosis s p emergent pericardial window via sternotomy cardiogenic shock tamponade s p min inv.',\n",
              " 'pfo closure s p cva anxiety depression cervical disc herniation patella femoral syndrome borderline hyperlipidemia discharge condition stable discharge instructions may shower over incision and gently pat dry no lotions creams or powders on incision no driving for one month no lifting greater than pounds for weeks call for fever greater than redness or drainage followup instructions follow up with dr. follow up with dr. follow up with dr. completed by']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent_tokenize(train_x_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "3LVbbFbDkshQ"
      },
      "outputs": [],
      "source": [
        "sent = []\n",
        "word_count = []\n",
        "sent_one = []\n",
        "for i in range(len(train_x_data)):\n",
        "  sent_one.append(sent_tokenize(train_x_data[i])) # i번째 document 기준\n",
        "  sent.append(len(sent_one[i])) # 각 document마다 나다타는 문장의 수를 보여줌 \n",
        "  for j in range(len(sent_one[i])):  # 각 document의 문장마다의 token개수를 word_count에 계속 추가함\n",
        "    word_count.append(len(sent_one[i][j]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "qLwmrJ-4rXEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent)) # train document의 숫자\n",
        "print(max(sent)) # DOCUMNET하나에서 나타나는 최대 문장개수는 452\n",
        "print(np.median(sent)) # 81.0\n",
        "print(np.mean(sent)) # DOCUMNET하나에서 나타나는 평균 문장개수는 86.58\n",
        "print(np.quantile(sent, 0.9)) # 150\n",
        "sns.displot(sent,  bins = 100, color = \"red\")  # 200아래에서 거의 존재함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "ps6yTdGNS7YG",
        "outputId": "0d852854-f9c0-423c-c767-c5575a675a8e"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "452\n",
            "81.0\n",
            "86.5865\n",
            "150.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f18ebd1ec90>"
            ]
          },
          "metadata": {},
          "execution_count": 159
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sent_one))\n",
        "print(len(word_count))  # 전체 document의 문장의수는 692692\n",
        "print(np.mean(word_count)) #문장마다의 평균 토큰수는 80.5\n",
        "print(max(word_count)) #문장에서 나타나는 최대 토큰수는 2323\n",
        "print(np.quantile(word_count, 0.9)) # 90quantile : 167.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vvWWZh0pdFU",
        "outputId": "a26a892e-38c5-4a10-e4a7-60f8dd1a7e8c"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "692692\n",
            "80.58640636819828\n",
            "2323\n",
            "167.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(word_count,  bins = 100, color = \"red\")  # 거의 대부분이 200이하임을 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "QvXyOOOAqCDB",
        "outputId": "2d928941-e252-4d6c-d053-ad27bf1e08ab"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f18ebcd0490>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxghy1x5sTco",
        "outputId": "1c196854-0367-41d7-c510-233567c097fb"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[287,\n",
              " 4,\n",
              " 50,\n",
              " 53,\n",
              " 33,\n",
              " 16,\n",
              " 41,\n",
              " 29,\n",
              " 4,\n",
              " 191,\n",
              " 26,\n",
              " 35,\n",
              " 39,\n",
              " 45,\n",
              " 29,\n",
              " 17,\n",
              " 111,\n",
              " 6,\n",
              " 5,\n",
              " 13,\n",
              " 3,\n",
              " 220,\n",
              " 42,\n",
              " 101,\n",
              " 34,\n",
              " 78,\n",
              " 31,\n",
              " 36,\n",
              " 76,\n",
              " 61,\n",
              " 73,\n",
              " 24,\n",
              " 70,\n",
              " 23,\n",
              " 38,\n",
              " 35,\n",
              " 16,\n",
              " 32,\n",
              " 11,\n",
              " 68,\n",
              " 23,\n",
              " 84,\n",
              " 23,\n",
              " 59,\n",
              " 23,\n",
              " 68,\n",
              " 23,\n",
              " 35,\n",
              " 28,\n",
              " 178,\n",
              " 455,\n",
              " 247,\n",
              " 37,\n",
              " 55,\n",
              " 89,\n",
              " 40,\n",
              " 13,\n",
              " 10,\n",
              " 29,\n",
              " 13,\n",
              " 18,\n",
              " 24,\n",
              " 25,\n",
              " 13,\n",
              " 12,\n",
              " 14,\n",
              " 12,\n",
              " 14,\n",
              " 16,\n",
              " 15,\n",
              " 46,\n",
              " 6,\n",
              " 85,\n",
              " 44,\n",
              " 47,\n",
              " 29,\n",
              " 10,\n",
              " 12,\n",
              " 12,\n",
              " 19,\n",
              " 39,\n",
              " 59,\n",
              " 27,\n",
              " 22,\n",
              " 87,\n",
              " 65,\n",
              " 71,\n",
              " 29,\n",
              " 12,\n",
              " 33,\n",
              " 28,\n",
              " 9,\n",
              " 99,\n",
              " 54,\n",
              " 144,\n",
              " 57,\n",
              " 83,\n",
              " 136,\n",
              " 26,\n",
              " 53,\n",
              " 42,\n",
              " 63,\n",
              " 40,\n",
              " 26,\n",
              " 164,\n",
              " 37,\n",
              " 16,\n",
              " 12,\n",
              " 28,\n",
              " 15,\n",
              " 17,\n",
              " 19,\n",
              " 21,\n",
              " 16,\n",
              " 21,\n",
              " 254,\n",
              " 44,\n",
              " 80,\n",
              " 165,\n",
              " 28,\n",
              " 256,\n",
              " 131,\n",
              " 151,\n",
              " 82,\n",
              " 87,\n",
              " 72,\n",
              " 107,\n",
              " 363,\n",
              " 41,\n",
              " 40,\n",
              " 183,\n",
              " 126,\n",
              " 125,\n",
              " 240,\n",
              " 113,\n",
              " 178,\n",
              " 121,\n",
              " 171,\n",
              " 151,\n",
              " 135,\n",
              " 107,\n",
              " 95,\n",
              " 24,\n",
              " 23,\n",
              " 13,\n",
              " 25,\n",
              " 27,\n",
              " 25,\n",
              " 14,\n",
              " 36,\n",
              " 28,\n",
              " 23,\n",
              " 17,\n",
              " 19,\n",
              " 21,\n",
              " 21,\n",
              " 6,\n",
              " 91,\n",
              " 93,\n",
              " 38,\n",
              " 76,\n",
              " 54,\n",
              " 117,\n",
              " 125,\n",
              " 28,\n",
              " 473,\n",
              " 22,\n",
              " 59,\n",
              " 95,\n",
              " 42,\n",
              " 69,\n",
              " 59,\n",
              " 24,\n",
              " 40,\n",
              " 66,\n",
              " 47,\n",
              " 28,\n",
              " 62,\n",
              " 25,\n",
              " 51,\n",
              " 19,\n",
              " 48,\n",
              " 50,\n",
              " 47,\n",
              " 11,\n",
              " 22,\n",
              " 37,\n",
              " 51,\n",
              " 24,\n",
              " 127,\n",
              " 31,\n",
              " 188,\n",
              " 160,\n",
              " 109,\n",
              " 111,\n",
              " 135,\n",
              " 114,\n",
              " 67,\n",
              " 84,\n",
              " 21,\n",
              " 78,\n",
              " 231,\n",
              " 87,\n",
              " 52,\n",
              " 23,\n",
              " 32,\n",
              " 53,\n",
              " 4,\n",
              " 40,\n",
              " 28,\n",
              " 408,\n",
              " 79,\n",
              " 103,\n",
              " 93,\n",
              " 82,\n",
              " 278,\n",
              " 59,\n",
              " 51,\n",
              " 36,\n",
              " 39,\n",
              " 66,\n",
              " 51,\n",
              " 64,\n",
              " 186,\n",
              " 134,\n",
              " 114,\n",
              " 136,\n",
              " 85,\n",
              " 43,\n",
              " 69,\n",
              " 30,\n",
              " 785,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 16,\n",
              " 6,\n",
              " 5,\n",
              " 10,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 16,\n",
              " 22,\n",
              " 9,\n",
              " 60,\n",
              " 83,\n",
              " 169,\n",
              " 59,\n",
              " 69,\n",
              " 259,\n",
              " 176,\n",
              " 127,\n",
              " 107,\n",
              " 55,\n",
              " 50,\n",
              " 61,\n",
              " 66,\n",
              " 50,\n",
              " 48,\n",
              " 71,\n",
              " 98,\n",
              " 44,\n",
              " 125,\n",
              " 30,\n",
              " 30,\n",
              " 65,\n",
              " 97,\n",
              " 184,\n",
              " 68,\n",
              " 98,\n",
              " 55,\n",
              " 115,\n",
              " 132,\n",
              " 69,\n",
              " 120,\n",
              " 34,\n",
              " 169,\n",
              " 79,\n",
              " 138,\n",
              " 44,\n",
              " 71,\n",
              " 70,\n",
              " 72,\n",
              " 54,\n",
              " 99,\n",
              " 120,\n",
              " 54,\n",
              " 66,\n",
              " 98,\n",
              " 66,\n",
              " 100,\n",
              " 92,\n",
              " 30,\n",
              " 62,\n",
              " 145,\n",
              " 78,\n",
              " 96,\n",
              " 75,\n",
              " 523,\n",
              " 35,\n",
              " 50,\n",
              " 227,\n",
              " 160,\n",
              " 26,\n",
              " 21,\n",
              " 15,\n",
              " 26,\n",
              " 124,\n",
              " 92,\n",
              " 72,\n",
              " 416,\n",
              " 356,\n",
              " 226,\n",
              " 70,\n",
              " 72,\n",
              " 138,\n",
              " 86,\n",
              " 83,\n",
              " 48,\n",
              " 120,\n",
              " 33,\n",
              " 35,\n",
              " 141,\n",
              " 57,\n",
              " 6,\n",
              " 48,\n",
              " 105,\n",
              " 70,\n",
              " 210,\n",
              " 82,\n",
              " 21,\n",
              " 15,\n",
              " 40,\n",
              " 38,\n",
              " 76,\n",
              " 52,\n",
              " 31,\n",
              " 59,\n",
              " 38,\n",
              " 101,\n",
              " 80,\n",
              " 144,\n",
              " 39,\n",
              " 27,\n",
              " 28,\n",
              " 42,\n",
              " 51,\n",
              " 33,\n",
              " 122,\n",
              " 38,\n",
              " 66,\n",
              " 18,\n",
              " 18,\n",
              " 18,\n",
              " 22,\n",
              " 33,\n",
              " 47,\n",
              " 124,\n",
              " 34,\n",
              " 49,\n",
              " 129,\n",
              " 126,\n",
              " 43,\n",
              " 70,\n",
              " 93,\n",
              " 6,\n",
              " 5,\n",
              " 11,\n",
              " 32,\n",
              " 145,\n",
              " 254,\n",
              " 48,\n",
              " 67,\n",
              " 114,\n",
              " 35,\n",
              " 17,\n",
              " 74,\n",
              " 56,\n",
              " 79,\n",
              " 200,\n",
              " 142,\n",
              " 265,\n",
              " 122,\n",
              " 118,\n",
              " 3,\n",
              " 73,\n",
              " 110,\n",
              " 164,\n",
              " 235,\n",
              " 99,\n",
              " 109,\n",
              " 85,\n",
              " 208,\n",
              " 68,\n",
              " 35,\n",
              " 245,\n",
              " 114,\n",
              " 64,\n",
              " 240,\n",
              " 90,\n",
              " 137,\n",
              " 109,\n",
              " 57,\n",
              " 122,\n",
              " 24,\n",
              " 104,\n",
              " 81,\n",
              " 37,\n",
              " 127,\n",
              " 117,\n",
              " 134,\n",
              " 49,\n",
              " 130,\n",
              " 63,\n",
              " 14,\n",
              " 19,\n",
              " 13,\n",
              " 248,\n",
              " 675,\n",
              " 280,\n",
              " 410,\n",
              " 81,\n",
              " 12,\n",
              " 76,\n",
              " 47,\n",
              " 43,\n",
              " 58,\n",
              " 159,\n",
              " 167,\n",
              " 51,\n",
              " 29,\n",
              " 107,\n",
              " 237,\n",
              " 12,\n",
              " 183,\n",
              " 17,\n",
              " 56,\n",
              " 1,\n",
              " 145,\n",
              " 7,\n",
              " 194,\n",
              " 4,\n",
              " 99,\n",
              " 20,\n",
              " 39,\n",
              " 31,\n",
              " 290,\n",
              " 25,\n",
              " 4,\n",
              " 66,\n",
              " 11,\n",
              " 73,\n",
              " 111,\n",
              " 35,\n",
              " 154,\n",
              " 246,\n",
              " 820,\n",
              " 106,\n",
              " 39,\n",
              " 188,\n",
              " 26,\n",
              " 75,\n",
              " 12,\n",
              " 390,\n",
              " 108,\n",
              " 35,\n",
              " 324,\n",
              " 168,\n",
              " 36,\n",
              " 49,\n",
              " 119,\n",
              " 131,\n",
              " 15,\n",
              " 44,\n",
              " 302,\n",
              " 16,\n",
              " 32,\n",
              " 114,\n",
              " 125,\n",
              " 125,\n",
              " 29,\n",
              " 117,\n",
              " 56,\n",
              " 63,\n",
              " 63,\n",
              " 211,\n",
              " 188,\n",
              " 284,\n",
              " 193,\n",
              " 23,\n",
              " 210,\n",
              " 133,\n",
              " 66,\n",
              " 15,\n",
              " 458,\n",
              " 48,\n",
              " 28,\n",
              " 63,\n",
              " 90,\n",
              " 104,\n",
              " 115,\n",
              " 83,\n",
              " 139,\n",
              " 108,\n",
              " 102,\n",
              " 31,\n",
              " 105,\n",
              " 38,\n",
              " 35,\n",
              " 16,\n",
              " 32,\n",
              " 11,\n",
              " 87,\n",
              " 23,\n",
              " 62,\n",
              " 23,\n",
              " 53,\n",
              " 23,\n",
              " 60,\n",
              " 23,\n",
              " 65,\n",
              " 23,\n",
              " 64,\n",
              " 23,\n",
              " 111,\n",
              " 42,\n",
              " 131,\n",
              " 51,\n",
              " 458,\n",
              " 284,\n",
              " 139,\n",
              " 106,\n",
              " 36,\n",
              " 177,\n",
              " 34,\n",
              " 12,\n",
              " 21,\n",
              " 207,\n",
              " 6,\n",
              " 5,\n",
              " 246,\n",
              " 16,\n",
              " 13,\n",
              " 365,\n",
              " 198,\n",
              " 74,\n",
              " 95,\n",
              " 104,\n",
              " 55,\n",
              " 54,\n",
              " 100,\n",
              " 84,\n",
              " 75,\n",
              " 88,\n",
              " 86,\n",
              " 130,\n",
              " 149,\n",
              " 35,\n",
              " 723,\n",
              " 49,\n",
              " 86,\n",
              " 99,\n",
              " 34,\n",
              " 81,\n",
              " 36,\n",
              " 101,\n",
              " 284,\n",
              " 76,\n",
              " 166,\n",
              " 57,\n",
              " 30,\n",
              " 56,\n",
              " 79,\n",
              " 96,\n",
              " 23,\n",
              " 70,\n",
              " 40,\n",
              " 31,\n",
              " 53,\n",
              " 36,\n",
              " 59,\n",
              " 46,\n",
              " 46,\n",
              " 47,\n",
              " 212,\n",
              " 35,\n",
              " 9,\n",
              " 15,\n",
              " 23,\n",
              " 10,\n",
              " 19,\n",
              " 52,\n",
              " 56,\n",
              " 29,\n",
              " 4,\n",
              " 9,\n",
              " 63,\n",
              " 37,\n",
              " 19,\n",
              " 7,\n",
              " 8,\n",
              " 24,\n",
              " 9,\n",
              " 8,\n",
              " 29,\n",
              " 23,\n",
              " 64,\n",
              " 17,\n",
              " 9,\n",
              " 98,\n",
              " 7,\n",
              " 9,\n",
              " 6,\n",
              " 5,\n",
              " 15,\n",
              " 11,\n",
              " 37,\n",
              " 6,\n",
              " 5,\n",
              " 10,\n",
              " 90,\n",
              " 230,\n",
              " 73,\n",
              " 64,\n",
              " 94,\n",
              " 38,\n",
              " 99,\n",
              " 56,\n",
              " 57,\n",
              " 70,\n",
              " 184,\n",
              " 177,\n",
              " 40,\n",
              " 29,\n",
              " 19,\n",
              " 170,\n",
              " 113,\n",
              " 66,\n",
              " 259,\n",
              " 60,\n",
              " 162,\n",
              " 111,\n",
              " 33,\n",
              " 19,\n",
              " 47,\n",
              " 179,\n",
              " 59,\n",
              " 92,\n",
              " 31,\n",
              " 39,\n",
              " 104,\n",
              " 85,\n",
              " 154,\n",
              " 80,\n",
              " 50,\n",
              " 151,\n",
              " 15,\n",
              " 28,\n",
              " 62,\n",
              " 91,\n",
              " 100,\n",
              " 32,\n",
              " 117,\n",
              " 210,\n",
              " 120,\n",
              " 38,\n",
              " 35,\n",
              " 368,\n",
              " 94,\n",
              " 35,\n",
              " 200,\n",
              " 35,\n",
              " 46,\n",
              " 362,\n",
              " 87,\n",
              " 558,\n",
              " 50,\n",
              " 278,\n",
              " 116,\n",
              " 26,\n",
              " 123,\n",
              " 88,\n",
              " 13,\n",
              " 108,\n",
              " 53,\n",
              " 45,\n",
              " 22,\n",
              " 69,\n",
              " 195,\n",
              " 44,\n",
              " 76,\n",
              " 73,\n",
              " 41,\n",
              " 56,\n",
              " 53,\n",
              " 22,\n",
              " 122,\n",
              " 276,\n",
              " 6,\n",
              " 5,\n",
              " 105,\n",
              " 22,\n",
              " 9,\n",
              " 9,\n",
              " 25,\n",
              " 9,\n",
              " 9,\n",
              " 158,\n",
              " 62,\n",
              " 202,\n",
              " 3,\n",
              " 253,\n",
              " 3,\n",
              " 3,\n",
              " 397,\n",
              " 82,\n",
              " 75,\n",
              " 101,\n",
              " 153,\n",
              " 108,\n",
              " 61,\n",
              " 241,\n",
              " 131,\n",
              " 86,\n",
              " 148,\n",
              " 135,\n",
              " 87,\n",
              " 84,\n",
              " 65,\n",
              " 149,\n",
              " 60,\n",
              " 64,\n",
              " 31,\n",
              " 154,\n",
              " 109,\n",
              " 101,\n",
              " 103,\n",
              " 33,\n",
              " 148,\n",
              " 274,\n",
              " 284,\n",
              " 107,\n",
              " 166,\n",
              " 85,\n",
              " 146,\n",
              " 56,\n",
              " 55,\n",
              " 71,\n",
              " 140,\n",
              " 49,\n",
              " 217,\n",
              " 47,\n",
              " 59,\n",
              " 88,\n",
              " 4,\n",
              " 265,\n",
              " 26,\n",
              " 22,\n",
              " 18,\n",
              " 33,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 9,\n",
              " 6,\n",
              " 5,\n",
              " 7,\n",
              " 14,\n",
              " 7,\n",
              " 11,\n",
              " 144,\n",
              " 46,\n",
              " 192,\n",
              " 86,\n",
              " 18,\n",
              " 79,\n",
              " 22,\n",
              " 9,\n",
              " 9,\n",
              " 11,\n",
              " 11,\n",
              " 9,\n",
              " 79,\n",
              " 5,\n",
              " 5,\n",
              " 5,\n",
              " 9,\n",
              " 6,\n",
              " 5,\n",
              " 11,\n",
              " 192,\n",
              " 219,\n",
              " 163,\n",
              " 48,\n",
              " 38,\n",
              " 58,\n",
              " 22,\n",
              " 94,\n",
              " 115,\n",
              " 58,\n",
              " 73,\n",
              " 85,\n",
              " 31,\n",
              " 90,\n",
              " 31,\n",
              " 19,\n",
              " 62,\n",
              " 98,\n",
              " 25,\n",
              " 39,\n",
              " 57,\n",
              " 148,\n",
              " 75,\n",
              " 269,\n",
              " 71,\n",
              " 280,\n",
              " 29,\n",
              " 32,\n",
              " 35,\n",
              " 49,\n",
              " 5,\n",
              " 518,\n",
              " 6,\n",
              " 5,\n",
              " 33,\n",
              " 37,\n",
              " 30,\n",
              " 74,\n",
              " 83,\n",
              " 63,\n",
              " 59,\n",
              " 38,\n",
              " 58,\n",
              " 25,\n",
              " 79,\n",
              " 31,\n",
              " 33,\n",
              " 79,\n",
              " 80,\n",
              " 14,\n",
              " 38,\n",
              " 13,\n",
              " 37,\n",
              " 44,\n",
              " 117,\n",
              " 74,\n",
              " 75,\n",
              " 85,\n",
              " 37,\n",
              " 103,\n",
              " 71,\n",
              " 43,\n",
              " 136,\n",
              " 76,\n",
              " 116,\n",
              " 69,\n",
              " 76,\n",
              " 122,\n",
              " 31,\n",
              " 25,\n",
              " 27,\n",
              " 23,\n",
              " 53,\n",
              " 23,\n",
              " 73,\n",
              " 24,\n",
              " 38,\n",
              " 35,\n",
              " 16,\n",
              " 32,\n",
              " 11,\n",
              " 70,\n",
              " 23,\n",
              " 64,\n",
              " 23,\n",
              " 74,\n",
              " 23,\n",
              " 65,\n",
              " 23,\n",
              " 69,\n",
              " 23,\n",
              " 52,\n",
              " 23,\n",
              " 52,\n",
              " 23,\n",
              " 59,\n",
              " 23,\n",
              " 67,\n",
              " 23,\n",
              " 66,\n",
              " 98,\n",
              " 5,\n",
              " 355,\n",
              " 43,\n",
              " 43,\n",
              " 36,\n",
              " 58,\n",
              " 164,\n",
              " 70,\n",
              " 49,\n",
              " 54,\n",
              " 232,\n",
              " 273,\n",
              " 81,\n",
              " 105,\n",
              " 48,\n",
              " 83,\n",
              " 40,\n",
              " 5,\n",
              " 36,\n",
              " 61,\n",
              " 10,\n",
              " 25,\n",
              " 12,\n",
              " 60,\n",
              " 58,\n",
              " 17,\n",
              " 72,\n",
              " 82,\n",
              " 83,\n",
              " 14,\n",
              " 17,\n",
              " 79,\n",
              " 28,\n",
              " 90,\n",
              " 160,\n",
              " 70,\n",
              " 170,\n",
              " 30,\n",
              " 23,\n",
              " 38,\n",
              " 35,\n",
              " 18,\n",
              " 32,\n",
              " 11,\n",
              " 65,\n",
              " 23,\n",
              " 64,\n",
              " 23,\n",
              " 59,\n",
              " 23,\n",
              " 54,\n",
              " 23,\n",
              " 106,\n",
              " 44,\n",
              " 102,\n",
              " 44,\n",
              " 12,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_one) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpf-GBOWn9AX",
        "outputId": "0093e076-cbda-4b6f-d655-4c10e1ad336f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['admission date date of birth service cardiothoracic allergies no known allergies adverse drug reactions attending chief complaint chest pain major surgical or invasive procedure procedures .', 'third time redo sternotomy.second time aortic valve replacement with a mm onyx mechanical valve reference number number .', 'redo cabg times with reverse saphenous vein graft from the neo ascending aorta to the preexisting saphenous vein graft to the right coronary artery reverse saphenous vein graft from the neo ascending aorta to the preexisting om vein graft.endoscopic vein harvesting.replacement of ascending aorta with a mm tube graft using deep hypothermic circulatory arrest .', 'sternal re exploration removal of packing and sternal closure history of present illness mr. with month of worsening cp on exertion and doe.', 'approximately month ago mr. doe which have been worsening over the last weeks.', 'approximately week ago he also noted resting shoulder pain and nocturnal cough.', 'pain responded to nitro at home.', 'he denies nausea lightheadedness dizziness leg swelling palps.', 'denies fevers chills sick contacts or cold symptoms.', 'does endorse a nonproductive cough for the last days.', 'denies recent increase in salt intake.in the ed initial vitals were .', 'ra.', 'labs significant for trop .', 'bnp twi in i avl v v .', 'cxr showed low lung volumes patchy focal infiltrate in lll which may indicate early pneumonia vs aspiration no pulmonary edema.', 'cta chest showed no pe b l pleural effusions and septal thickening c w fluid overload bibasilar consolidations atelectasis vs pneumonia.', 'he developed an episode of cp in the ed that resolved with sl nitro.', 'patient was given asa mg lasix mg iv levofloxacin mg iv nitro .', 'mg sl x .', 'blood cultures were drawn.', 'vitals on transfer were .', 'nsr rr l nc.', 'he diuresed cc of urine to mg iv of lasix.on arrival to the floor patient is comfortable without cp or sob feels well.on review of systems he denies any prior history of stroke tia deep venous thrombosis pulmonary embolism bleeding at the time of surgery myalgias joint pains hemoptysis or red stools.', 'he does endorese black stools x several months.', 'he denies recent fevers chills or rigors.', 'he denies exertional buttock or calf pain.', 'all of the other review of systems were negative.cardiac review of systems is notable for absence of orthopnea ankle edema palpitations syncope or presyncope.', 'past medical history .', 'cardiac risk factors dyslipidemia hypertension .', 'cardiac history cabg cabg x in percutaneous coronary interventions des to lima pacing icd n a .', 'other past medical history avr atrial fibrillation rhythm controlled occurred s p valve replacement .', 'social history divorced.', 'lives with companion of years maintenance worker for tobacco history quit years ago previously smoked ppd x years etoh highballs night illicit drugs denies family history father died suddenly at age of unknown cause.', 'brother with cad s p cabg at age .', 'physical exam vs t= .', 'bp= hr= rr= o sat= l nc .', 'kg general wdwn male in distress muscles oriented x .', 'mood affect appropriate.', 'heent ncat.', 'sclera anicteric.', 'perrl eomi.', 'conjunctiva were pink no pallor or cyanosis of the oral mucosa.', 'no xanthalesma.', 'neck supple with jvp of cm +hjr.', 'cardiac rr normal s s click.', 'iii vi holosystolic murmur without radiation to carotids.', 'no thrills lifts.', 'no s or s .', 'lungs diminished bs at the bases insp crackles above that on l no wheezes or rhonchi.', 'no chest wall deformities scoliosis or kyphosis.', 'resp was labored.', 'abdomen soft ntnd.', 'no hsm or tenderness.', 'abd aorta not enlarged by palpation.', 'no abdominal bruits.', 'extremities trace pedal edema b l worse on r l svg was obtained from rle .', 'no femoral bruits.', 'skin no stasis dermatitis ulcers scars or xanthomas.', 'pulses right carotid + femoral + popliteal + dp + pt + left carotid + femoral + popliteal + dp + pt + pertinent results mcv mch .', 'mchc .', 'rdw .', 'plt ct baso .', 'tee no spontaneous echo contrast or thrombus is seen in the body of the left atrium or left atrial appendage.', 'there is moderate to severe regional left ventricular systolic dysfunction with ef with moderate inferoseptal wall hypokinesis.', 'the appearance of the ascending aorta is consistent with a normal tube graft.', 'a bileaflet aortic valve prosthesis is present.', 'the aortic valve prosthesis leaflets appear to move normally.', 'the transaortic gradient is normal for this prosthesis.', 'there is no aortic valve stenosis.', '[the amount of regurgitation present is normal for this prosthetic aortic valve.]', 'no abnormal perivalvular leak is appreciated.', 'the mitral valve leaflets are mildly thickened.', 'mild + mitral regurgitation is seen.', 'there is no pericardial effusion.', 'brief hospital course m with complex cardiac history.', 'he underwent cabg in a redo sternotomy avr in chest pain.', 'he presented to the ed with an nstemi and new onset chf on stenosis with a valve area .cm .', 'cath revealed multi vessel cad.', 'cardiac surgery is consulted for rd time sternotomy avr cabg evaluation.', 'on .', 'third time redo sternotomy.second time aortic valve replacement with a mm onyx mechanical valve reference number number .', 'redo cabg times with reverse saphenous vein graft from the neo ascending aorta to the preexisting saphenous vein graft to the right coronary artery reverse saphenous vein graft from the neo ascending aorta to the preexisting om vein graft.endoscopic vein harvesting.replacement of ascending aorta with a mm tube graft using deep hypothermic circulatory arrest.', 'his chest was left open at the end of the case despite multiple blood products the patient was coagulopathic and dr. decided to pack the chest and the the chest and the chest open.', 'the patient was taken directly from the or to the icu for ongoing post op care and management.', 'he was on milrinone epi vasopressin neo and propofol.', 'he was taken back tothe operating room on pod and his chest was closed.', 'he was weaned and extubated on pod .', 'he was weaned off his inotropes and was hypertensive and refractory to po anti hypertensives and started on a nicardipine drip.', 'the oral antihypertensives were increased and the nicardipine was weaned off.', 'he was started on coumadin for mechcanical avr.', 'he was started on statin ace betablocker and lasix therapies.', 'despite aggressive diuresis he continued to have firm taut edema of his lower extremities.', 'he continued to progress quickly and was transferred to the stepdown unit on pod .', 'he was evaluated by physical therapy for strength and conditoning and discharge to rehab was recommended.', 'he was noted to have mild erythema and scant serous drainage of his right svh incision site on pod and he was discharged to rehab medications on admission sotalol mg verapamil er mg daily imdur mg daily crestor mg daily asa mg daily fish oil cap daily vit c mg po daily vit d units daily discharge medications .', 'metoprolol tartrate mg tablet sig .', 'tablets po tid times a day .potassium chloride meq tablet extended release sig four tablet extended release po q h every hours .docusate sodium mg capsule sig one capsule po bid times a day .aspirin mg tablet delayed release e.c.', 'sig one tablet delayed release e.c.', 'po daily daily .acetaminophen mg tablet sig two tablet po q h every hours as needed for pain fever.magnesium hydroxide mg ml suspension sig thirty ml po hs at bedtime as needed for constipation.bisacodyl mg suppository sig one suppository rectal daily daily as needed for constipation.clonidine .', 'mg tablet sig one tablet po tid times a day .amlodipine mg tablet sig two tablet po daily daily .pantoprazole mg tablet delayed release e.c.', 'sig one tablet delayed release e.c.', 'po q h every hours .furosemide mg tablet sig one tablet po bid times a day until edema resolves.tramadol mg tablet sig one tablet po q h every hours as needed for pain.', 'disp tablet s refills .', 'rosuvastatin mg tablet sig one tablet po daily daily .warfarin mg tablet sig one tablet po daily daily indication mech avr goal inr .', 'daily coumadin based on inr.outpatient lab work follow inr daily until therapeutic then times weekly until stable next draw .', 'potassium chloride meq tablet extended release sig one tablet extended release po once a day while on lasix but follow bun creat and potassium levels times weekly.cephalexin mg tablet sig one tablet po q h every hours for days for erythema of leg incision.', 'discharge disposition extended care facility discharge diagnosis .', 'prosthetic aortic valve stenosis.severe vessel coronary disease.severe disease of vein grafts from previous coronary artery bypass grafting.peripheral vascular disease.open chest status post rd time redo coronary artery bypass grafting aortic valve replacement and replacement of ascending aorta yesterday with post procedure coagulopathy.', 'discharge condition alert and oriented x nonfocal ambulating with steady gait but deconditoned incisional pain managed with incisions sternal healing well no erythema or drainage leg right healing with slight erythema and scant serous drainage keflex started x days.', 'edema taut firm edema to bilateral lower extremities.', 'discharge instructions please shower daily including washing incisions gently with mild soap no baths or swimming until cleared by surgeon.', 'look at your incisions daily for redness or drainage please no lotions cream powder or ointments to incisions each morning you should weigh yourself and then in the evening take your temperature these should be written down on the chart no driving for approximately one month and while taking narcotics will be discussed at follow up appointment with surgeon when you will be able to drive no lifting more than pounds for weeks please call with any questions or concerns please call cardiac surgery office with any questions or concerns person during off hours followup instructions you are scheduled for the following appointments surgeon date time please call to schedule appointments with your primary care cardiologist chakraborty aurobindo please call cardiac surgery office with any questions or concerns person during off hours labs pt inr for coumadin ??????', 'indication mech avr goal inr .', 'first draw results to phone fax please arrange coumadin follow up upon discharge from rehab completed by']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(word_count) # 80.58640636819828"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yJzOQ3DnDMr",
        "outputId": "62440cd1-280f-4139-ba0a-825998920b37"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80.58640636819828"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_count) # 총문장의 개수는 692692"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swNhEwrSnVnZ",
        "outputId": "46dd5a6a-b2f1-4641-fae2-e49f343133b0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "692692"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_token = tokenizer.texts_to_sequences(sent_one)\n",
        "len(word_token[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD5NiOTdgCFn",
        "outputId": "e34bd4d3-57f1-49c9-c87d-86875193a311"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_token[0])\n",
        "print(len(word_token[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa2k35Quj65v",
        "outputId": "b50101f1-b4f2-4c73-a737-01629ebdefb4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[40, 61, 61, 3, 206, 100, 1029, 130, 12, 279, 130, 1407, 460, 1409, 237, 259, 247, 63, 41, 244, 139, 18, 239, 136, 2416]\n",
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKLiegLkjqCT",
        "outputId": "a3c1303f-81ca-4199-9380-7c40faa7620b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_one"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eWfLqLCdpfP",
        "outputId": "3526f15b-8461-4323-bf54-93e0166ef341"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['admission date date of birth service cardiothoracic allergies no known allergies adverse drug reactions attending chief complaint chest pain major surgical or invasive procedure procedures .',\n",
              " 'third time redo sternotomy.second time aortic valve replacement with a mm onyx mechanical valve reference number number .',\n",
              " 'redo cabg times with reverse saphenous vein graft from the neo ascending aorta to the preexisting saphenous vein graft to the right coronary artery reverse saphenous vein graft from the neo ascending aorta to the preexisting om vein graft.endoscopic vein harvesting.replacement of ascending aorta with a mm tube graft using deep hypothermic circulatory arrest .',\n",
              " 'sternal re exploration removal of packing and sternal closure history of present illness mr. with month of worsening cp on exertion and doe.',\n",
              " 'approximately month ago mr. doe which have been worsening over the last weeks.',\n",
              " 'approximately week ago he also noted resting shoulder pain and nocturnal cough.',\n",
              " 'pain responded to nitro at home.',\n",
              " 'he denies nausea lightheadedness dizziness leg swelling palps.',\n",
              " 'denies fevers chills sick contacts or cold symptoms.',\n",
              " 'does endorse a nonproductive cough for the last days.',\n",
              " 'denies recent increase in salt intake.in the ed initial vitals were .',\n",
              " 'ra.',\n",
              " 'labs significant for trop .',\n",
              " 'bnp twi in i avl v v .',\n",
              " 'cxr showed low lung volumes patchy focal infiltrate in lll which may indicate early pneumonia vs aspiration no pulmonary edema.',\n",
              " 'cta chest showed no pe b l pleural effusions and septal thickening c w fluid overload bibasilar consolidations atelectasis vs pneumonia.',\n",
              " 'he developed an episode of cp in the ed that resolved with sl nitro.',\n",
              " 'patient was given asa mg lasix mg iv levofloxacin mg iv nitro .',\n",
              " 'mg sl x .',\n",
              " 'blood cultures were drawn.',\n",
              " 'vitals on transfer were .',\n",
              " 'nsr rr l nc.',\n",
              " 'he diuresed cc of urine to mg iv of lasix.on arrival to the floor patient is comfortable without cp or sob feels well.on review of systems he denies any prior history of stroke tia deep venous thrombosis pulmonary embolism bleeding at the time of surgery myalgias joint pains hemoptysis or red stools.',\n",
              " 'he does endorese black stools x several months.',\n",
              " 'he denies recent fevers chills or rigors.',\n",
              " 'he denies exertional buttock or calf pain.',\n",
              " 'all of the other review of systems were negative.cardiac review of systems is notable for absence of orthopnea ankle edema palpitations syncope or presyncope.',\n",
              " 'past medical history .',\n",
              " 'cardiac risk factors dyslipidemia hypertension .',\n",
              " 'cardiac history cabg cabg x in percutaneous coronary interventions des to lima pacing icd n a .',\n",
              " 'other past medical history avr atrial fibrillation rhythm controlled occurred s p valve replacement .',\n",
              " 'social history divorced.',\n",
              " 'lives with companion of years maintenance worker for tobacco history quit years ago previously smoked ppd x years etoh highballs night illicit drugs denies family history father died suddenly at age of unknown cause.',\n",
              " 'brother with cad s p cabg at age .',\n",
              " 'physical exam vs t= .',\n",
              " 'bp= hr= rr= o sat= l nc .',\n",
              " 'kg general wdwn male in distress muscles oriented x .',\n",
              " 'mood affect appropriate.',\n",
              " 'heent ncat.',\n",
              " 'sclera anicteric.',\n",
              " 'perrl eomi.',\n",
              " 'conjunctiva were pink no pallor or cyanosis of the oral mucosa.',\n",
              " 'no xanthalesma.',\n",
              " 'neck supple with jvp of cm +hjr.',\n",
              " 'cardiac rr normal s s click.',\n",
              " 'iii vi holosystolic murmur without radiation to carotids.',\n",
              " 'no thrills lifts.',\n",
              " 'no s or s .',\n",
              " 'lungs diminished bs at the bases insp crackles above that on l no wheezes or rhonchi.',\n",
              " 'no chest wall deformities scoliosis or kyphosis.',\n",
              " 'resp was labored.',\n",
              " 'abdomen soft ntnd.',\n",
              " 'no hsm or tenderness.',\n",
              " 'abd aorta not enlarged by palpation.',\n",
              " 'no abdominal bruits.',\n",
              " 'extremities trace pedal edema b l worse on r l svg was obtained from rle .',\n",
              " 'no femoral bruits.',\n",
              " 'skin no stasis dermatitis ulcers scars or xanthomas.',\n",
              " 'pulses right carotid + femoral + popliteal + dp + pt + left carotid + femoral + popliteal + dp + pt + pertinent results mcv mch .',\n",
              " 'mchc .',\n",
              " 'rdw .',\n",
              " 'plt ct baso .',\n",
              " 'tee no spontaneous echo contrast or thrombus is seen in the body of the left atrium or left atrial appendage.',\n",
              " 'there is moderate to severe regional left ventricular systolic dysfunction with ef with moderate inferoseptal wall hypokinesis.',\n",
              " 'the appearance of the ascending aorta is consistent with a normal tube graft.',\n",
              " 'a bileaflet aortic valve prosthesis is present.',\n",
              " 'the aortic valve prosthesis leaflets appear to move normally.',\n",
              " 'the transaortic gradient is normal for this prosthesis.',\n",
              " 'there is no aortic valve stenosis.',\n",
              " '[the amount of regurgitation present is normal for this prosthetic aortic valve.]',\n",
              " 'no abnormal perivalvular leak is appreciated.',\n",
              " 'the mitral valve leaflets are mildly thickened.',\n",
              " 'mild + mitral regurgitation is seen.',\n",
              " 'there is no pericardial effusion.',\n",
              " 'brief hospital course m with complex cardiac history.',\n",
              " 'he underwent cabg in a redo sternotomy avr in chest pain.',\n",
              " 'he presented to the ed with an nstemi and new onset chf on stenosis with a valve area .cm .',\n",
              " 'cath revealed multi vessel cad.',\n",
              " 'cardiac surgery is consulted for rd time sternotomy avr cabg evaluation.',\n",
              " 'on .',\n",
              " 'third time redo sternotomy.second time aortic valve replacement with a mm onyx mechanical valve reference number number .',\n",
              " 'redo cabg times with reverse saphenous vein graft from the neo ascending aorta to the preexisting saphenous vein graft to the right coronary artery reverse saphenous vein graft from the neo ascending aorta to the preexisting om vein graft.endoscopic vein harvesting.replacement of ascending aorta with a mm tube graft using deep hypothermic circulatory arrest.',\n",
              " 'his chest was left open at the end of the case despite multiple blood products the patient was coagulopathic and dr. decided to pack the chest and the the chest and the chest open.',\n",
              " 'the patient was taken directly from the or to the icu for ongoing post op care and management.',\n",
              " 'he was on milrinone epi vasopressin neo and propofol.',\n",
              " 'he was taken back tothe operating room on pod and his chest was closed.',\n",
              " 'he was weaned and extubated on pod .',\n",
              " 'he was weaned off his inotropes and was hypertensive and refractory to po anti hypertensives and started on a nicardipine drip.',\n",
              " 'the oral antihypertensives were increased and the nicardipine was weaned off.',\n",
              " 'he was started on coumadin for mechcanical avr.',\n",
              " 'he was started on statin ace betablocker and lasix therapies.',\n",
              " 'despite aggressive diuresis he continued to have firm taut edema of his lower extremities.',\n",
              " 'he continued to progress quickly and was transferred to the stepdown unit on pod .',\n",
              " 'he was evaluated by physical therapy for strength and conditoning and discharge to rehab was recommended.',\n",
              " 'he was noted to have mild erythema and scant serous drainage of his right svh incision site on pod and he was discharged to rehab medications on admission sotalol mg verapamil er mg daily imdur mg daily crestor mg daily asa mg daily fish oil cap daily vit c mg po daily vit d units daily discharge medications .',\n",
              " 'metoprolol tartrate mg tablet sig .',\n",
              " 'tablets po tid times a day .potassium chloride meq tablet extended release sig four tablet extended release po q h every hours .docusate sodium mg capsule sig one capsule po bid times a day .aspirin mg tablet delayed release e.c.',\n",
              " 'sig one tablet delayed release e.c.',\n",
              " 'po daily daily .acetaminophen mg tablet sig two tablet po q h every hours as needed for pain fever.magnesium hydroxide mg ml suspension sig thirty ml po hs at bedtime as needed for constipation.bisacodyl mg suppository sig one suppository rectal daily daily as needed for constipation.clonidine .',\n",
              " 'mg tablet sig one tablet po tid times a day .amlodipine mg tablet sig two tablet po daily daily .pantoprazole mg tablet delayed release e.c.',\n",
              " 'sig one tablet delayed release e.c.',\n",
              " 'po q h every hours .furosemide mg tablet sig one tablet po bid times a day until edema resolves.tramadol mg tablet sig one tablet po q h every hours as needed for pain.',\n",
              " 'disp tablet s refills .',\n",
              " 'rosuvastatin mg tablet sig one tablet po daily daily .warfarin mg tablet sig one tablet po daily daily indication mech avr goal inr .',\n",
              " 'daily coumadin based on inr.outpatient lab work follow inr daily until therapeutic then times weekly until stable next draw .',\n",
              " 'potassium chloride meq tablet extended release sig one tablet extended release po once a day while on lasix but follow bun creat and potassium levels times weekly.cephalexin mg tablet sig one tablet po q h every hours for days for erythema of leg incision.',\n",
              " 'discharge disposition extended care facility discharge diagnosis .',\n",
              " 'prosthetic aortic valve stenosis.severe vessel coronary disease.severe disease of vein grafts from previous coronary artery bypass grafting.peripheral vascular disease.open chest status post rd time redo coronary artery bypass grafting aortic valve replacement and replacement of ascending aorta yesterday with post procedure coagulopathy.',\n",
              " 'discharge condition alert and oriented x nonfocal ambulating with steady gait but deconditoned incisional pain managed with incisions sternal healing well no erythema or drainage leg right healing with slight erythema and scant serous drainage keflex started x days.',\n",
              " 'edema taut firm edema to bilateral lower extremities.',\n",
              " 'discharge instructions please shower daily including washing incisions gently with mild soap no baths or swimming until cleared by surgeon.',\n",
              " 'look at your incisions daily for redness or drainage please no lotions cream powder or ointments to incisions each morning you should weigh yourself and then in the evening take your temperature these should be written down on the chart no driving for approximately one month and while taking narcotics will be discussed at follow up appointment with surgeon when you will be able to drive no lifting more than pounds for weeks please call with any questions or concerns please call cardiac surgery office with any questions or concerns person during off hours followup instructions you are scheduled for the following appointments surgeon date time please call to schedule appointments with your primary care cardiologist chakraborty aurobindo please call cardiac surgery office with any questions or concerns person during off hours labs pt inr for coumadin ??????',\n",
              " 'indication mech avr goal inr .',\n",
              " 'first draw results to phone fax please arrange coumadin follow up upon discharge from rehab completed by']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(sent))\n",
        "sent_df = pd.DataFrame(sent)\n",
        "sent_df.shape #(8000, 1) \n",
        "sent_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "4_MJ6WUPTQIl",
        "outputId": "00810d3f-7d0d-4b49-f305-f3c38a97fd3a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0\n",
              "0      51\n",
              "1      69\n",
              "2      45\n",
              "3      45\n",
              "4     153\n",
              "...   ...\n",
              "7995   93\n",
              "7996   69\n",
              "7997  118\n",
              "7998  155\n",
              "7999  114\n",
              "\n",
              "[8000 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19fac414-e7a1-462d-b4bd-4042ca293748\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7995</th>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7996</th>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7997</th>\n",
              "      <td>118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7998</th>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7999</th>\n",
              "      <td>114</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19fac414-e7a1-462d-b4bd-4042ca293748')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-19fac414-e7a1-462d-b4bd-4042ca293748 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-19fac414-e7a1-462d-b4bd-4042ca293748');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyfb237HlJf8",
        "outputId": "a0c7a66d-74d6-4fc1-86fd-756f678ebc94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "452\n",
            "86.5865\n",
            "1\n",
            "8000\n"
          ]
        }
      ],
      "source": [
        "print(max(sent)) # 452 : 각 document 1개 안에 있는 문장의 개수\n",
        "print(np.mean(sent)) # 90.2226 : 평균적인 길이\n",
        "print(min(sent)) # 1\n",
        "print(len(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGRJrAxmgVpx"
      },
      "source": [
        "원본 : \"Admission Date:  [**2135-5-9**]              Discharge Date:   [**2135-5-13**]\n",
        "\n",
        "Date of Birth:  [**2087-7-14**]             Sex:   F\n",
        "\n",
        "Service: CARDIOTHORACIC\n",
        "\n",
        "Allergies:\n",
        "Bactrim / Ampicillin / Remeron\n",
        "\n",
        "Attending:[**First Name3 (LF) 1283**]\n",
        "Chief Complaint:\n",
        "chest pressure/cardiac tamponade/ cardiogenic shock\n",
        "\n",
        "Major Surgical or Invasive Procedure:\n",
        "emergent sternotomy for pericardial window  [**2135-5-9**]\n",
        "\n",
        "\n",
        "History of Present Illness:\n",
        "Underwent min. inv. PFO closure in [**12-11**]. Had emergent admission\n",
        "on [**5-9**] for hypotension, pericardial effusion , pleural effusion\n",
        "and chest pain for several days. Did not resolve with pain med\n",
        "and had increasing SOB. Admitted to ER for emergent eval. and\n",
        "bedside TTE. Started on dopamine drip for hypotension.\n",
        "\n",
        "Past Medical History:\n",
        "s/p min. inv. closure of Patent foramen ovale [**12-11**]; History of\n",
        "Stroke/TIA; Depression; Anxiety; Borderline Hyperlipidemia;\n",
        "Herniation of Cervical Discs; Patella-Femoral Syndrome; s/p\n",
        "Bunionectomies\n",
        "\n",
        "Social History:\n",
        "Denies tobacco. Admits to occasional ETOH. She is an employee of\n",
        "the [**Hospital1 18**] in the Neuro-Pysch Department. She is married with two\n",
        "children. She denies IVDA and recreational drugs.\n",
        "\n",
        "\n",
        "Family History:\n",
        "Father underwent CABG at age 72. Cousin died of an MI at age 46.\n",
        "\n",
        "Physical Exam:\n",
        "pt. in distress\n",
        "SBP 70- 80's\n",
        "lungs CTA\n",
        "tachycardic, RR, no murmur or rubs\n",
        "palpable pedal pulses\n",
        "\n",
        "Pertinent Results:\n",
        "[**2135-5-11**] 08:40AM BLOOD WBC-11.3* RBC-3.62* Hgb-9.9* Hct-28.8*\n",
        "MCV-80* MCH-27.3 MCHC-34.3 RDW-14.4 Plt Ct-413\n",
        "[**2135-5-9**] 11:45AM BLOOD Neuts-86.5* Lymphs-7.0* Monos-5.2 Eos-1.2\n",
        "Baso-0.2\n",
        "[**2135-5-11**] 08:40AM BLOOD Plt Ct-413\n",
        "[**2135-5-11**] 08:40AM BLOOD Glucose-118* UreaN-8 Creat-0.6 Na-136\n",
        "K-3.7 Cl-102 HCO3-24 AnGap-14\n",
        "[**2135-5-9**] 11:45AM BLOOD CK(CPK)-26\n",
        "[**2135-5-9**] 11:45AM BLOOD CK-MB-NotDone cTropnT-<0.01\n",
        "\n",
        "Brief Hospital Course:\n",
        "Admitted through ER as above and referred to CT [**Doctor First Name **] for\n",
        "emergent pericardial window/pericardectomy via sternotomy, as\n",
        "the patient was hypotensive.This was performed by Dr. [**Last Name (STitle) 1290**]\n",
        "on [**5-9**]. Transferred to CSRU in stable condition on phenylephrine\n",
        "and propofol drips. Extubated and awoke neurologically intact.\n",
        "Beta blockade started on POD #1 and transferred out to the floor\n",
        "to start increasing her activity level. Mediastinal tubes\n",
        "removed on POD #1. Crepitus was noted on anterior chest wall\n",
        "after pleural tubes removed on POD #2. Beta blockade also\n",
        "titrated up. Crepitus improved and CXR confirmed. She made good\n",
        "progress and was discharged to home with VNA services on POD #4.\n",
        "\n",
        "Medications on Admission:\n",
        "ASA 325 mg daily\n",
        "\n",
        "\n",
        "Discharge Medications:\n",
        "1. Docusate Sodium 100 mg Capsule Sig: One (1) Capsule PO BID (2\n",
        "times a day) for 1 months.\n",
        "Disp:*60 Capsule(s)* Refills:*0*\n",
        "2. Ranitidine HCl 150 mg Tablet Sig: One (1) Tablet PO BID (2\n",
        "times a day) for 1 months.\n",
        "Disp:*60 Tablet(s)* Refills:*0*\n",
        "3. Aspirin 81 mg Tablet, Delayed Release (E.C.) Sig: One (1)\n",
        "Tablet, Delayed Release (E.C.) PO DAILY (Daily).\n",
        "Disp:*30 Tablet, Delayed Release (E.C.)(s)* Refills:*2*\n",
        "4. Acetaminophen 325 mg Tablet Sig: Two (2) Tablet PO Q4H (every\n",
        "4 hours) as needed.\n",
        "Disp:*30 Tablet(s)* Refills:*0*\n",
        "5. Oxycodone-Acetaminophen 5-325 mg Tablet Sig: 1-2 Tablets PO\n",
        "Q4H (every 4 hours) as needed for pain.\n",
        "Disp:*40 Tablet(s)* Refills:*0*\n",
        "6. Furosemide 20 mg Tablet Sig: One (1) Tablet PO once a day for\n",
        "3 days.\n",
        "Disp:*3 Tablet(s)* Refills:*0*\n",
        "7. Potassium Chloride 20 mEq Packet Sig: One (1) Packet PO once\n",
        "a day for 3 days.\n",
        "Disp:*3 Packet(s)* Refills:*0*\n",
        "8. Metoprolol Tartrate 25 mg Tablet Sig: 1.5 Tablets PO BID (2\n",
        "times a day).\n",
        "Disp:*90 Tablet(s)* Refills:*1*\n",
        "\n",
        "\n",
        "Discharge Disposition:\n",
        "Home With Service\n",
        "\n",
        "Facility:\n",
        "[**Company 1519**]\n",
        "\n",
        "Discharge Diagnosis:\n",
        "s/p emergent pericardial window via sternotomy [**2135-5-9**]\n",
        "cardiogenic shock/tamponade\n",
        "s/p Min inv. PFO closure [**12-11**]\n",
        "s/p CVA\n",
        "anxiety/depression\n",
        "cervical disc herniation\n",
        "patella-femoral syndrome\n",
        "borderline hyperlipidemia\n",
        "\n",
        "\n",
        "Discharge Condition:\n",
        "stable\n",
        "\n",
        "\n",
        "Discharge Instructions:\n",
        "may shower over incision and gently pat dry\n",
        "no lotions, creams or powders on incision\n",
        "no driving for one month\n",
        "no lifting greater than 10 pounds for 10 weeks\n",
        "call for fever greater than 100, redness or drainage\n",
        "\n",
        "Followup Instructions:\n",
        "follow up with Dr. [**Last Name (STitle) **] (PCP) in [**2-7**] weeks\n",
        "follow up with Dr. [**Last Name (STitle) **] (Card)in [**3-11**] weeks\n",
        "follow up with Dr. [**Last Name (STitle) 1290**] in 4 weeks   [**Telephone/Fax (1) 170**]\n",
        "\n",
        "\n",
        "\n",
        "Completed by:[**2135-5-13**]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9sKRejedISW"
      },
      "outputs": [],
      "source": [
        "def doc2hierarchical(text,\n",
        "                     max_sentences=MAX_SENTENCES,  # 위에서 정의한값\n",
        "                     max_sentence_length=MAX_SENTENCE_LENGTH):  # 위에서 정의한값\n",
        "    sentences = sent_tokenize(text)  # 긴문장을 문장 단위로 쪼개줌\n",
        "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "    tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=max_sentence_length)  # 패딩을 써서 길이를 맞춰춤\n",
        "\n",
        "    pad_size = max_sentences - tokenized_sentences.shape[0]\n",
        "\n",
        "    if pad_size <= 0:  # tokenized_sentences.shape[0] < max_sentences\n",
        "        tokenized_sentences = tokenized_sentences[:max_sentences]  # 패딩사이즈를 넘어가면 중간에 잘라주어야함\n",
        "    else:\n",
        "        tokenized_sentences = np.pad(\n",
        "            tokenized_sentences, ((0, pad_size), (0, 0)),\n",
        "            mode='constant', constant_values=0\n",
        "        )\n",
        "\n",
        "    return tokenized_sentences\n",
        "\n",
        "\n",
        "def build_dataset(x_data, y_data,\n",
        "                  max_sentences=MAX_SENTENCES,\n",
        "                  max_sentence_length=MAX_SENTENCE_LENGTH,\n",
        "                  tokenizer=tokenizer):\n",
        "    nb_instances = len(x_data)\n",
        "    X_data = np.zeros((nb_instances, max_sentences, max_sentence_length), dtype='int32')\n",
        "    for i, review in enumerate(x_data):\n",
        "        tokenized_sentences = doc2hierarchical(review)\n",
        "\n",
        "        X_data[i] = tokenized_sentences[None, ...]\n",
        "\n",
        "    nb_classes = len(set(y_data))  # 0,1 : 2개 카테고리 > 2값이 나옴\n",
        "    Y_data = to_categorical(y_data, nb_classes)  # 1/0 binary를 2개의 컬럼으로 나타냄 / one-hot encoding과 같음\n",
        "    # Y_data = y_data\n",
        "    return X_data, Y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUS5kvTPmXkc",
        "outputId": "c2587659-05a1-4e39-e618-4094d222f822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(train_y_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xxc5WMjh-5D",
        "outputId": "d24daf14-bded-49ee-d724-00e6a064e270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_X_data.shape: (8000, 300, 50)\n",
            "test_X_data.shape: (4808, 300, 50)\n",
            "train_Y_data.shape: (8000, 2)\n",
            "test_Y_data.shape: (4808, 2)\n"
          ]
        }
      ],
      "source": [
        "train_X_data, train_Y_data = build_dataset(train_x_data, train_y_data)\n",
        "test_X_data, test_Y_data = build_dataset(test_x_data, test_y_data)\n",
        "\n",
        "train_X_data[1][1]\n",
        "train_X_data[1]\n",
        "\n",
        "print(\"train_X_data.shape: {}\".format(train_X_data.shape)) # (10000, 300, 30)\n",
        "print(\"test_X_data.shape: {}\".format(test_X_data.shape)) # \n",
        "print(\"train_Y_data.shape: {}\".format(train_Y_data.shape)) # (10000, 300, 30)\n",
        "print(\"test_Y_data.shape: {}\".format(test_Y_data.shape)) # "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmclXXxllyR6",
        "outputId": "8bd9fe23-bdb8-4f8e-8538-00b052819050"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       ...,\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_Y_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s-gEsGKkKmg",
        "outputId": "b3086165-3ac6-4a5f-9e26-4083ca2c5443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300, 50)"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X_data[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfxjZY_PM0c8",
        "outputId": "2da614bd-f1cf-4715-bb7f-b512267e8f19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[[    0,     0,     0, ...,   189,   399,  1200],\n",
              "        [    0,     0,     0, ...,     0,     0, 15756],\n",
              "        [    0,     0,     0, ...,    11,   420,    94],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       [[    0,     0,     0, ...,     1, 11876,   551],\n",
              "        [    0,     0,     0, ...,    18,   569,    41],\n",
              "        [    0,     0,     0, ...,   538,   352,    71],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       [[    0,     0,     0, ...,    10,   101,    83],\n",
              "        [    0,     0,     0, ...,     3,    63,    41],\n",
              "        [    0,     0,     0, ...,  1772,  2130,   118],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[    0,    40,    61, ...,  4440, 10931,  3593],\n",
              "        [    0,     0,     0, ...,  1692,   434,  1890],\n",
              "        [    0,     0,     0, ...,    17,   656,  1612],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       [[   61,    61,     3, ...,   457,     8,   392],\n",
              "        [    0,     0,     0, ...,  5419,  2801,   522],\n",
              "        [    0,     0,     0, ...,   534,     5,  2686],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       [[    0,     0,     0, ...,   239,   136,  2416],\n",
              "        [    0,     0,     0, ...,  3320,   833,   833],\n",
              "        [  470,   534,    46, ...,  5756,  9100,  1198],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]]], dtype=int32)"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG1xv_P950Dm",
        "outputId": "d5aa1b2f-1979-4e4b-bdb5-f3e2e982e96c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_Y_data[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rdbjxyW6UD_",
        "outputId": "471369b5-9d3d-45b3-f7ca-7b4a552de103"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8000, 2)"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_Y_data.shape # (800, 50, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoUoP7vekQVk",
        "outputId": "07c45b03-7a9f-4298-c31f-2e13d03df800"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[    0,     0,     0, ...,     1, 11876,   551],\n",
              "       [    0,     0,     0, ...,    18,   569,    41],\n",
              "       [    0,     0,     0, ...,   538,   352,    71],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,     0,     0,     0],\n",
              "       [    0,     0,     0, ...,     0,     0,     0],\n",
              "       [    0,     0,     0, ...,     0,     0,     0]], dtype=int32)"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X_data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi76V7KdkUAH",
        "outputId": "d24930e8-2e3d-4286-e5da-6c621ea6fbac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_X_data[1][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GdfYPpDh9iA",
        "outputId": "e1783798-65ae-4d67-ce8d-12c62ceec0cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_X_data.shape: (7200, 300, 50)\n",
            "train_Y_data.shape: (7200, 2)\n",
            "val_X_data.shape: (800, 300, 50)\n",
            "val_Y_data.shape: (800, 2)\n"
          ]
        }
      ],
      "source": [
        "# train-valid test split (9:1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "# train -> train과 valid로 나누기\n",
        "train_X_data, val_X_data, train_Y_data, val_Y_data = train_test_split(train_X_data, train_Y_data, test_size=0.1,random_state=42)\n",
        "print(\"train_X_data.shape: {}\".format(train_X_data.shape))\n",
        "print(\"train_Y_data.shape: {}\".format(train_Y_data.shape))\n",
        "print(\"val_X_data.shape: {}\".format(val_X_data.shape))\n",
        "print(\"val_Y_data.shape: {}\".format(val_Y_data.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fn5qdJJ2eimf"
      },
      "source": [
        "# word2vec embedding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGcQqHDNp7dV",
        "outputId": "74c9e3f7-2469-43b4-f174-4cc09b31e06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_matrix.shape: (63994, 300)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(63994, 300)"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# google이 이미 word2vec의 결과로 각 word에 대한 vector를 만들어서 배포\n",
        "# 하지만 용량이 커서 작은 용량에서는 어렵죠. 그래서 작은 용량의 word2vec vector로 존재\n",
        "# vector를 특정 분야에 맞게 특화시키킬 원할 경우,\n",
        "# Word2vec.intersect_word2vec_format(googleNews_filepath, binary=True, lockf=1.0)을 통해 쉽게 초기값을 설정\n",
        "# word2vec-GoogleNews-vectors에서 이미 학습된 word2vector를 다운가능\n",
        "# 다만, 학습된 모델을 가져오는 것이 아니라, “학습된 vector”만을 수치로 가져오는 것이죠.\n",
        "# 다운로드 받아놓은게 있어서 따로 다운받을 필요 없을듯\n",
        "#import gensim.downloader as api\n",
        "#wv = api.load('word2vec-google-news-300')\n",
        "#vec_king = wv['king']\n",
        "\n",
        "#embedding_dir = '/home/kjk88/TBM_DBLAC', # 파일 넣은 위치\n",
        "\n",
        "# word2vec을 가져옴\n",
        "def load_word2vec(tokenizer=tokenizer):\n",
        "    from gensim.models import KeyedVectors\n",
        "    embedding_path = '/content/gdrive/MyDrive/dataset/word2vec/GoogleNews-vectors-negative300.bin'  # embedding_dir 위치에 다운로드 받은 파일을 넣기 다운받는데 시간이 좀 걸림\n",
        "    # embedding_path = os.path.join(embedding_dir,'GoogleNews-vectors-negative300.bin')\n",
        "    # https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz 해당링크에서 파일 다운로드 1.5GB정도됨\n",
        "    embeddings_index = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
        "\n",
        "    return embeddings_index  # embedding_index 를 불러옴, 예를들면 dog라는 단어가 있으면 dog의  index 500 을 가져옴\n",
        "\n",
        "\n",
        "def load_embedding(embedding_type='word2vec',\n",
        "                   tokenizer=tokenizer,\n",
        "                   embedding_dim=300):\n",
        "    if embedding_type == 'word2vec':\n",
        "        embeddings_index = load_word2vec()  # 위에서 언급한 함수가 사용됨 load_word2vec\n",
        "\n",
        "    embedding_matrix = np.random.normal(0, 1, (max_nb_words, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        try:\n",
        "            embedding_vector = embeddings_index[word]\n",
        "        except KeyError:\n",
        "            embedding_vector = None\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix\n",
        "    # dog라른 단어가 있으면 해당 index를 구하고 해당 index를 찾아서 이에 대한 벡터를 가져옴\n",
        "    # 전체 사용한 단어에 대해서 이에 대한 벡터 매트릭스를 생성해줌\n",
        "    # 여기서는 124253단어에 대해서 해당 벡터를 전부 가져올 수 있음\n",
        "\n",
        "embedding_matrix = load_embedding('word2vec')\n",
        "\n",
        "print(\"embedding_matrix.shape: {}\".format(embedding_matrix.shape)) # (652, 300)\n",
        "embedding_matrix # 총 63994 개의 단어들을 각각 300차원으로 만들어서 표현함\n",
        "embedding_matrix.shape #(63994, 300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICOT194ZvudX",
        "outputId": "714dfa16-7fbe-4694-8011-c720955f5019"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.18121326,  0.80678311,  1.51455157, ..., -0.12010355,\n",
              "        -0.41548121,  0.2588295 ],\n",
              "       [ 0.08007812,  0.10498047,  0.04980469, ...,  0.00366211,\n",
              "         0.04760742, -0.06884766],\n",
              "       [ 0.07440108,  0.8755278 , -0.1559248 , ..., -0.66754624,\n",
              "        -0.19618672, -0.4099477 ],\n",
              "       ...,\n",
              "       [ 1.05447101,  1.49177174,  2.02222064, ..., -0.27773957,\n",
              "         0.38075151, -1.09616873],\n",
              "       [ 0.75366611,  1.25770479,  1.31101535, ...,  0.55714529,\n",
              "        -1.08523258,  1.14489413],\n",
              "       [-1.40338528,  1.27151336, -0.72773532, ..., -1.29574431,\n",
              "         0.07013679, -1.22718285]])"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf2Wlr6Mel8V"
      },
      "source": [
        "# evaluation metrics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPyDzZ-zvV_l"
      },
      "outputs": [],
      "source": [
        "# evaluation function 3가지\n",
        "def recall(y_target, y_pred):  # 2개 요소 실제값, 예측값 필요\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1))  # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1))  # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn)\n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
        "\n",
        "    # TP = y_target_yn * y_pred_yn\n",
        "    # FN = y_target_yn - (y_target_yn * y_pred_yn)\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "def precision(y_target, y_pred):  # 2개 요소 실제값, 예측값 필요\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = K.round(K.clip(y_pred, 0, 1))  # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = K.round(K.clip(y_target, 0, 1))  # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = K.sum(y_target_yn * y_pred_yn)\n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
        "\n",
        "    # FP = y_pred_yn - (y_target_yn * y_pred_yn)\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1score(y_target, y_pred):  # 2개 요소 실제값, 예측값 필요\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = (2 * _recall * _precision) / (_recall + _precision + K.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return _f1score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSyNbE6ZeqHm"
      },
      "source": [
        "# layers setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ESo7JnPNFhw"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(Layer):  # Layer를 가져옴\n",
        "    def __init__(self, attention_dim, **kwargs):  # **kwargs : list가져올때 씀\n",
        "        self.attention_dim = attention_dim\n",
        "        super(AttentionLayer, self).__init__(**kwargs)  # attentionlayer의 속성을 가져옴\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='Attention_Weight',  # attention 값 이값을 확인해야하는것 같음\n",
        "                                 shape=(input_shape[-1], self.attention_dim),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)  # 학습가능\n",
        "        self.b = self.add_weight(name='Attention_Bias',\n",
        "                                 shape=(self.attention_dim,),  # attention값과 쌍을 이룸?\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.u = self.add_weight(name='Attention_Context_Vector',  # u: context vector\n",
        "                                 shape=(self.attention_dim, 1),  # attention값과 쌍을 이룸?\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        # refer to the original paper\n",
        "        # link: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
        "        u_it = K.tanh(K.dot(x, self.W) + self.b)  # hiddenlayer와의 계산\n",
        "        a_it = K.dot(u_it, self.u)  # a가 attention을 의미함 word attention이라고 볼 수 있음\n",
        "        a_it = K.squeeze(a_it, -1)  # 일자형태로 만들기\n",
        "        a_it = K.softmax(a_it)  # softmax함수 적용 합이1이 되게함\n",
        "\n",
        "        return a_it\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1])\n",
        "\n",
        "\n",
        "def WeightedSum(attentions, representations):  #\n",
        "    # from Shape(batch_size, len_units) to Shape(batch_size, rnn_dim * 2, len_units)\n",
        "    repeated_attentions = RepeatVector(K.int_shape(representations)[-1])(attentions)\n",
        "    # from Shape(batch_size, rnn_dim * 2, len_units) to Shape(batch_size, len_units, lstm_dim * 2)\n",
        "    repeated_attentions = Permute([2, 1])(repeated_attentions)\n",
        "\n",
        "    # compute representation as the weighted sum of representations\n",
        "    aggregated_representation = Multiply()([representations, repeated_attentions])\n",
        "    aggregated_representation = Lambda(lambda x: K.sum(x, axis=1))(aggregated_representation)\n",
        "\n",
        "    return aggregated_representation  # 합쳐진 representation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df97i5pqNGQd"
      },
      "outputs": [],
      "source": [
        "def HieAtt(embedding_matrix,  # word2vec한 값\n",
        "           max_sentences,\n",
        "           max_sentence_length,\n",
        "           nb_classes,  # 클래스의 개수 여기서는 2개\n",
        "           embedding_dim=300,  # 임베딩 차원 여기서는 300차원으로 변환시킴\n",
        "           attention_dim=100,\n",
        "           rnn_dim=150,\n",
        "           include_dense_batch_normalization=False,\n",
        "           include_dense_dropout=True,\n",
        "           nb_dense=1,\n",
        "           dense_dim=300,\n",
        "           dense_dropout=0.2,\n",
        "           optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)):\n",
        "    # Use tf.keras.optimizers.Adam(learning_rate) instead of keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    # embedding_matrix = (max_nb_words + 1, embedding_dim)\n",
        "    max_nb_words = embedding_matrix.shape[0] - 1  # 124253-1\n",
        "    embedding_layer = Embedding(max_nb_words + 1,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=max_sentence_length,  # input\n",
        "                                trainable=False)  # embedding한걸 다시 학습할 필요없음\n",
        "\n",
        "    # first, build a sentence encoder\n",
        "    sentence_input = Input(shape=(max_sentence_length,), dtype='int32')\n",
        "    embedded_sentence = embedding_layer(sentence_input)  # sentence에 대한 임베딩 / sentence단위의 임베딩으로 한단계 위\n",
        "    embedded_sentence = Dropout(dense_dropout)(embedded_sentence)\n",
        "    contextualized_sentence = Bidirectional(GRU(rnn_dim, return_sequences=True))(embedded_sentence)  # CuDNNGRU\n",
        "    # embedded sentence 를 bidirectional featrue2개가 생성되고 두개가 합쳐짐\n",
        "\n",
        "    # word attention computation\n",
        "    word_attention = AttentionLayer(attention_dim)(contextualized_sentence)  # 위에서 설정한 attentionlayer함수사용\n",
        "    sentence_representation = WeightedSum(word_attention, contextualized_sentence)  # 위에서 설정한 weightedsum 함수사용\n",
        "\n",
        "    sentence_encoder = Model(inputs=[sentence_input],  # from keras.models import Model,\n",
        "                             outputs=[sentence_representation])  # sentence를 벡터로 표현하는 과정\n",
        "\n",
        "    # then, build a document encoder (최종 아웃풋임)\n",
        "    document_input = Input(shape=(max_sentences, max_sentence_length), dtype='int32')\n",
        "    embedded_document = TimeDistributed(sentence_encoder)(document_input)\n",
        "    contextualized_document = Bidirectional(GRU(rnn_dim, return_sequences=True))(embedded_document)  # CuDNNGRU\n",
        "    # sentence를 이용해서 doc에 대한 representation v계산\n",
        "\n",
        "    # sentence attention computation\n",
        "    sentence_attention = AttentionLayer(attention_dim)(contextualized_document)\n",
        "    # 해당 코드에서 가장 중요한 부분 : sentence_attention  각 sentence별 attention값이 계산되는것을 확인\n",
        "    document_representation = WeightedSum(sentence_attention, contextualized_document)\n",
        "    # doc에 대해 하나로 표현가능한 vector값이 나오게됨\n",
        "\n",
        "    # 마지막 fc layer를 통해서 분류문제를 해결하고자 함\n",
        "    # finally, add fc layers for classification\n",
        "    fc_layers = Sequential()\n",
        "    for _ in range(nb_dense):  # 클래스 수만큼 fc layer를 수행함\n",
        "        if include_dense_batch_normalization == True:  # batch norm 을 한경우\n",
        "            fc_layers.add(BatchNormalization())\n",
        "        fc_layers.add(Dense(dense_dim, activation='relu'))\n",
        "        if include_dense_dropout == True:  # drop_out 을 한경우\n",
        "            fc_layers.add(Dropout(dense_dropout))\n",
        "    fc_layers.add(Dense(nb_classes, activation='softmax'))  # soft max\n",
        "\n",
        "    pred_sentiment = fc_layers(document_representation)  # 최종 결과값이라 볼 수 있음\n",
        "\n",
        "    model = Model(inputs=[document_input],\n",
        "                  outputs=[pred_sentiment])\n",
        "\n",
        "    ############### build attention extractor ###############\n",
        "    word_attention_extractor = Model(inputs=[sentence_input],\n",
        "                                     outputs=[word_attention])\n",
        "    word_attentions = TimeDistributed(word_attention_extractor)(document_input)\n",
        "    attention_extractor = Model(inputs=[document_input],\n",
        "                                outputs=[word_attentions, sentence_attention])\n",
        "    # 모델에 대한 마지막 정의\n",
        "    model.compile(loss=['categorical_crossentropy'],\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model, attention_extractor, sentence_attention, word_attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMV6vTK8eusU"
      },
      "source": [
        "# model - HAN "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOy9Qymnsq77"
      },
      "outputs": [],
      "source": [
        "model_name = \"HieAtt\"\n",
        "model_path = '/content/checkpoints/{}.h5'.format(model_name)\n",
        "checkpointer = ModelCheckpoint(filepath=model_path,\n",
        "                               monitor='val_acc',\n",
        "                               verbose=True,\n",
        "                               save_best_only=True,\n",
        "                               mode='max')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvbfw1krwDCl",
        "outputId": "303e089c-1e09-41fe-8737-560ccd9f738e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_10 (InputLayer)          [(None, 300, 50)]    0           []                               \n",
            "                                                                                                  \n",
            " time_distributed_8 (TimeDistri  (None, 300, 300)    19635200    ['input_10[0][0]']               \n",
            " buted)                                                                                           \n",
            "                                                                                                  \n",
            " bidirectional_9 (Bidirectional  (None, 300, 300)    406800      ['time_distributed_8[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " attention_layer_9 (AttentionLa  (None, 300)         30200       ['bidirectional_9[0][0]']        \n",
            " yer)                                                                                             \n",
            "                                                                                                  \n",
            " repeat_vector_9 (RepeatVector)  (None, 300, 300)    0           ['attention_layer_9[0][0]']      \n",
            "                                                                                                  \n",
            " permute_9 (Permute)            (None, 300, 300)     0           ['repeat_vector_9[0][0]']        \n",
            "                                                                                                  \n",
            " multiply_9 (Multiply)          (None, 300, 300)     0           ['bidirectional_9[0][0]',        \n",
            "                                                                  'permute_9[0][0]']              \n",
            "                                                                                                  \n",
            " lambda_9 (Lambda)              (None, 300)          0           ['multiply_9[0][0]']             \n",
            "                                                                                                  \n",
            " sequential_4 (Sequential)      (None, 2)            90902       ['lambda_9[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 20,163,102\n",
            "Trainable params: 964,902\n",
            "Non-trainable params: 19,198,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model, attention_extractor, sentence_att, word_att = HieAtt(embedding_matrix=embedding_matrix,\n",
        "                                    max_sentences=MAX_SENTENCES,\n",
        "                                    max_sentence_length=MAX_SENTENCE_LENGTH,\n",
        "                                    nb_classes=2,\n",
        "                                    embedding_dim=300,\n",
        "                                    attention_dim=100,\n",
        "                                    rnn_dim=150,\n",
        "                                    include_dense_batch_normalization=False,\n",
        "                                    include_dense_dropout=True,\n",
        "                                    nb_dense=1,\n",
        "                                    dense_dim=300,\n",
        "                                    dense_dropout=0.2,\n",
        "                                    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "                                    )\n",
        "\n",
        "\n",
        "model.summary()  # OUTPUT SHAPE를 확인 가능\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQUiEsFVwU30",
        "outputId": "63b821d4-a8b5-4139-8e5b-abc6751f6b1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_data=(val_X_data, val_Y_data) ;type(validation_data) # TUPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnL21QljOZq9",
        "outputId": "901996c7-7d00-4ee4-f0ab-2b3230fd8226"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7200, 300, 50)"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X_data.shape # (720, 300, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tql9jdeT7oGJ",
        "outputId": "c1c1b26a-4419-4374-b17b-9a0a64435c3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "97571"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 실제 데이터에 적용하는 과정\n",
        "# Hyperparameter\n",
        "max_sentences = 300\n",
        "max_sentence_length = 50  # maxlen이랑 같음\n",
        "# 사전에서 단어 수(embedding layer에서 사용)\n",
        "max_nb_words = len(tokenizer.word_index) + 1\n",
        "max_nb_words # 63994\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIp7VV0u7gQ",
        "outputId": "99754e75-ae1d-44a2-f2be-78089c146292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "72/72 [==============================] - 901s 12s/step - loss: 0.3171 - accuracy: 0.9040 - val_loss: 0.2832 - val_accuracy: 0.9187\n",
            "Epoch 2/10\n",
            "72/72 [==============================] - 891s 12s/step - loss: 0.2907 - accuracy: 0.9129 - val_loss: 0.2655 - val_accuracy: 0.9187\n",
            "Epoch 3/10\n",
            "72/72 [==============================] - 890s 12s/step - loss: 0.2713 - accuracy: 0.9143 - val_loss: 0.1977 - val_accuracy: 0.9187\n",
            "Epoch 4/10\n",
            "72/72 [==============================] - 892s 12s/step - loss: 0.1822 - accuracy: 0.9364 - val_loss: 0.1431 - val_accuracy: 0.9513\n",
            "Epoch 5/10\n",
            "72/72 [==============================] - 895s 12s/step - loss: 0.1593 - accuracy: 0.9417 - val_loss: 0.1348 - val_accuracy: 0.9463\n",
            "Epoch 6/10\n",
            "72/72 [==============================] - 894s 12s/step - loss: 0.1399 - accuracy: 0.9461 - val_loss: 0.1038 - val_accuracy: 0.9488\n",
            "Epoch 7/10\n",
            "72/72 [==============================] - 892s 12s/step - loss: 0.1237 - accuracy: 0.9485 - val_loss: 0.1062 - val_accuracy: 0.9525\n",
            "Epoch 8/10\n",
            "72/72 [==============================] - 895s 12s/step - loss: 0.1143 - accuracy: 0.9524 - val_loss: 0.1078 - val_accuracy: 0.9550\n",
            "Epoch 9/10\n",
            "72/72 [==============================] - 894s 12s/step - loss: 0.1059 - accuracy: 0.9569 - val_loss: 0.1052 - val_accuracy: 0.9575\n",
            "Epoch 10/10\n",
            "72/72 [==============================] - 897s 12s/step - loss: 0.0978 - accuracy: 0.9617 - val_loss: 0.0941 - val_accuracy: 0.9588\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x=train_X_data,  # x : list\n",
        "                    y=train_Y_data,  # y: list\n",
        "                    batch_size= 100, # 128\n",
        "                    epochs=10,  # 100\n",
        "                    verbose=True,\n",
        "                    validation_data=(val_X_data, val_Y_data)  # tuple , 'NoneType' object is not callable\n",
        "                    #,callbacks=[early_stop]\n",
        "                    )\n",
        "\n",
        "\n",
        "# error\n",
        "# def _fixup_shape(images, labels):\n",
        "#    images.set_shape([None, 15, 256, 256, 3])\n",
        "#    labels.set_shape([None, 12])\n",
        "#    return images, labels\n",
        "# as_list() is not defined on an unknown tensorshape\n",
        "\n",
        "#model.load_weights(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65xgY688LfGG",
        "outputId": "b0617562-d012-4dff-a489-605eb08ae4d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "72/72 [==============================] - 910s 13s/step - loss: 0.6528 - accuracy: 0.6212 - val_loss: 0.6410 - val_accuracy: 0.6087\n",
            "Epoch 2/20\n",
            "72/72 [==============================] - 907s 13s/step - loss: 0.6306 - accuracy: 0.6313 - val_loss: 0.6112 - val_accuracy: 0.6550\n",
            "Epoch 3/20\n",
            "72/72 [==============================] - 907s 13s/step - loss: 0.5972 - accuracy: 0.6735 - val_loss: 0.5642 - val_accuracy: 0.7275\n",
            "Epoch 4/20\n",
            "72/72 [==============================] - 908s 13s/step - loss: 0.5073 - accuracy: 0.7514 - val_loss: 0.4883 - val_accuracy: 0.7588\n",
            "Epoch 5/20\n",
            "72/72 [==============================] - 908s 13s/step - loss: 0.4716 - accuracy: 0.7790 - val_loss: 0.4564 - val_accuracy: 0.7850\n",
            "Epoch 6/20\n",
            "72/72 [==============================] - 909s 13s/step - loss: 0.4439 - accuracy: 0.7942 - val_loss: 0.4730 - val_accuracy: 0.7862\n",
            "Epoch 7/20\n",
            "72/72 [==============================] - 910s 13s/step - loss: 0.4238 - accuracy: 0.8069 - val_loss: 0.4487 - val_accuracy: 0.7887\n",
            "Epoch 8/20\n",
            "72/72 [==============================] - 912s 13s/step - loss: 0.4031 - accuracy: 0.8175 - val_loss: 0.4367 - val_accuracy: 0.8000\n",
            "Epoch 9/20\n",
            "72/72 [==============================] - 910s 13s/step - loss: 0.3751 - accuracy: 0.8361 - val_loss: 0.4647 - val_accuracy: 0.8037\n",
            "Epoch 10/20\n",
            "72/72 [==============================] - 911s 13s/step - loss: 0.3424 - accuracy: 0.8499 - val_loss: 0.4546 - val_accuracy: 0.8050\n",
            "Epoch 11/20\n",
            "72/72 [==============================] - 911s 13s/step - loss: 0.3243 - accuracy: 0.8561 - val_loss: 0.4685 - val_accuracy: 0.8075\n",
            "Epoch 12/20\n",
            "72/72 [==============================] - 913s 13s/step - loss: 0.3006 - accuracy: 0.8693 - val_loss: 0.5149 - val_accuracy: 0.7862\n",
            "Epoch 13/20\n",
            "72/72 [==============================] - 913s 13s/step - loss: 0.2698 - accuracy: 0.8850 - val_loss: 0.4932 - val_accuracy: 0.7788\n",
            "Epoch 14/20\n",
            "72/72 [==============================] - 913s 13s/step - loss: 0.2585 - accuracy: 0.8878 - val_loss: 0.5144 - val_accuracy: 0.7937\n",
            "Epoch 15/20\n",
            "72/72 [==============================] - 915s 13s/step - loss: 0.2296 - accuracy: 0.9058 - val_loss: 0.6005 - val_accuracy: 0.7862\n",
            "Epoch 16/20\n",
            "72/72 [==============================] - 913s 13s/step - loss: 0.2085 - accuracy: 0.9153 - val_loss: 0.6420 - val_accuracy: 0.7862\n",
            "Epoch 17/20\n",
            "72/72 [==============================] - 916s 13s/step - loss: 0.1958 - accuracy: 0.9176 - val_loss: 0.5945 - val_accuracy: 0.7912\n",
            "Epoch 18/20\n",
            "72/72 [==============================] - 916s 13s/step - loss: 0.1766 - accuracy: 0.9261 - val_loss: 0.6639 - val_accuracy: 0.7837\n",
            "Epoch 19/20\n",
            "72/72 [==============================] - 916s 13s/step - loss: 0.1569 - accuracy: 0.9390 - val_loss: 0.6942 - val_accuracy: 0.7900\n",
            "Epoch 20/20\n",
            "72/72 [==============================] - 915s 13s/step - loss: 0.1290 - accuracy: 0.9501 - val_loss: 0.7281 - val_accuracy: 0.7775\n"
          ]
        }
      ],
      "source": [
        "# \n",
        "\n",
        "history = model.fit(x=train_X_data,  # x : list\n",
        "                    y=train_Y_data,  # y: list\n",
        "                    batch_size= 100, # 128\n",
        "                    epochs=20,  # 100\n",
        "                    verbose=True,\n",
        "                    validation_data=(val_X_data, val_Y_data)  # tuple , 'NoneType' object is not callable\n",
        "                    #,callbacks=[early_stop]\n",
        "                    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J88DSoT6WVHP",
        "outputId": "6291e539-e09f-47b0-a2f6-1a080ef04f4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "144/144 [==============================] - 116s 723ms/step - loss: 0.6656 - accuracy: 0.6217 - val_loss: 0.6269 - val_accuracy: 0.6413\n",
            "Epoch 2/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.6036 - accuracy: 0.6722 - val_loss: 0.6516 - val_accuracy: 0.6538\n",
            "Epoch 3/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.5624 - accuracy: 0.7069 - val_loss: 0.5084 - val_accuracy: 0.7287\n",
            "Epoch 4/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.4840 - accuracy: 0.7686 - val_loss: 0.4949 - val_accuracy: 0.7437\n",
            "Epoch 5/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.4544 - accuracy: 0.7906 - val_loss: 0.4512 - val_accuracy: 0.7812\n",
            "Epoch 6/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.4214 - accuracy: 0.8103 - val_loss: 0.4381 - val_accuracy: 0.7925\n",
            "Epoch 7/100\n",
            "144/144 [==============================] - 102s 712ms/step - loss: 0.3905 - accuracy: 0.8275 - val_loss: 0.4414 - val_accuracy: 0.7850\n",
            "Epoch 8/100\n",
            "144/144 [==============================] - 103s 712ms/step - loss: 0.3600 - accuracy: 0.8421 - val_loss: 0.4797 - val_accuracy: 0.7750\n",
            "Epoch 9/100\n",
            "144/144 [==============================] - 103s 712ms/step - loss: 0.3307 - accuracy: 0.8561 - val_loss: 0.4973 - val_accuracy: 0.7788\n",
            "Epoch 10/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.3125 - accuracy: 0.8678 - val_loss: 0.5097 - val_accuracy: 0.7937\n",
            "Epoch 11/100\n",
            "144/144 [==============================] - 102s 712ms/step - loss: 0.2852 - accuracy: 0.8767 - val_loss: 0.5142 - val_accuracy: 0.7763\n",
            "Epoch 12/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.2429 - accuracy: 0.9031 - val_loss: 0.6469 - val_accuracy: 0.7825\n",
            "Epoch 13/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.2296 - accuracy: 0.9054 - val_loss: 0.5510 - val_accuracy: 0.7862\n",
            "Epoch 14/100\n",
            "144/144 [==============================] - 102s 712ms/step - loss: 0.1947 - accuracy: 0.9215 - val_loss: 0.6598 - val_accuracy: 0.7725\n",
            "Epoch 15/100\n",
            "144/144 [==============================] - 103s 712ms/step - loss: 0.1710 - accuracy: 0.9299 - val_loss: 0.6297 - val_accuracy: 0.7900\n",
            "Epoch 16/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.1433 - accuracy: 0.9442 - val_loss: 0.6509 - val_accuracy: 0.7650\n",
            "Epoch 17/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.1309 - accuracy: 0.9497 - val_loss: 0.7562 - val_accuracy: 0.7613\n",
            "Epoch 18/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.1087 - accuracy: 0.9590 - val_loss: 0.9115 - val_accuracy: 0.7862\n",
            "Epoch 19/100\n",
            "144/144 [==============================] - 103s 712ms/step - loss: 0.0950 - accuracy: 0.9629 - val_loss: 0.8267 - val_accuracy: 0.7763\n",
            "Epoch 20/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0911 - accuracy: 0.9651 - val_loss: 0.8498 - val_accuracy: 0.7750\n",
            "Epoch 21/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0760 - accuracy: 0.9729 - val_loss: 0.8451 - val_accuracy: 0.7812\n",
            "Epoch 22/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0756 - accuracy: 0.9715 - val_loss: 0.9625 - val_accuracy: 0.7738\n",
            "Epoch 23/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0657 - accuracy: 0.9743 - val_loss: 0.8233 - val_accuracy: 0.7713\n",
            "Epoch 24/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0526 - accuracy: 0.9799 - val_loss: 0.9886 - val_accuracy: 0.7900\n",
            "Epoch 25/100\n",
            "144/144 [==============================] - 102s 712ms/step - loss: 0.0439 - accuracy: 0.9833 - val_loss: 1.0160 - val_accuracy: 0.7987\n",
            "Epoch 26/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0464 - accuracy: 0.9844 - val_loss: 1.0396 - val_accuracy: 0.7975\n",
            "Epoch 27/100\n",
            "144/144 [==============================] - 102s 712ms/step - loss: 0.0395 - accuracy: 0.9850 - val_loss: 1.0519 - val_accuracy: 0.8000\n",
            "Epoch 28/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0452 - accuracy: 0.9847 - val_loss: 1.1520 - val_accuracy: 0.7775\n",
            "Epoch 29/100\n",
            "144/144 [==============================] - 102s 708ms/step - loss: 0.0408 - accuracy: 0.9864 - val_loss: 1.0364 - val_accuracy: 0.7925\n",
            "Epoch 30/100\n",
            "144/144 [==============================] - 101s 705ms/step - loss: 0.0313 - accuracy: 0.9887 - val_loss: 1.2184 - val_accuracy: 0.7950\n",
            "Epoch 31/100\n",
            "144/144 [==============================] - 101s 705ms/step - loss: 0.0384 - accuracy: 0.9876 - val_loss: 1.0462 - val_accuracy: 0.7950\n",
            "Epoch 32/100\n",
            "144/144 [==============================] - 102s 706ms/step - loss: 0.0387 - accuracy: 0.9846 - val_loss: 1.2054 - val_accuracy: 0.7925\n",
            "Epoch 33/100\n",
            "144/144 [==============================] - 102s 705ms/step - loss: 0.0334 - accuracy: 0.9903 - val_loss: 1.1097 - val_accuracy: 0.7912\n",
            "Epoch 34/100\n",
            "144/144 [==============================] - 102s 707ms/step - loss: 0.0387 - accuracy: 0.9864 - val_loss: 1.0964 - val_accuracy: 0.7800\n",
            "Epoch 35/100\n",
            "144/144 [==============================] - 102s 706ms/step - loss: 0.0274 - accuracy: 0.9908 - val_loss: 1.2396 - val_accuracy: 0.7975\n",
            "Epoch 36/100\n",
            "144/144 [==============================] - 102s 706ms/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 1.1216 - val_accuracy: 0.7812\n",
            "Epoch 37/100\n",
            "144/144 [==============================] - 102s 706ms/step - loss: 0.0255 - accuracy: 0.9911 - val_loss: 1.1893 - val_accuracy: 0.7763\n",
            "Epoch 38/100\n",
            "144/144 [==============================] - 101s 704ms/step - loss: 0.0320 - accuracy: 0.9896 - val_loss: 1.0590 - val_accuracy: 0.7663\n",
            "Epoch 39/100\n",
            "144/144 [==============================] - 101s 704ms/step - loss: 0.0212 - accuracy: 0.9921 - val_loss: 1.1963 - val_accuracy: 0.7812\n",
            "Epoch 40/100\n",
            "144/144 [==============================] - 102s 706ms/step - loss: 0.0328 - accuracy: 0.9867 - val_loss: 1.2687 - val_accuracy: 0.7912\n",
            "Epoch 41/100\n",
            "144/144 [==============================] - 102s 706ms/step - loss: 0.0347 - accuracy: 0.9868 - val_loss: 1.1551 - val_accuracy: 0.7750\n",
            "Epoch 42/100\n",
            "144/144 [==============================] - 102s 708ms/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 1.3734 - val_accuracy: 0.7800\n",
            "Epoch 43/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0378 - accuracy: 0.9865 - val_loss: 1.2154 - val_accuracy: 0.7887\n",
            "Epoch 44/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0226 - accuracy: 0.9928 - val_loss: 1.1845 - val_accuracy: 0.7837\n",
            "Epoch 45/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0169 - accuracy: 0.9947 - val_loss: 1.3537 - val_accuracy: 0.7800\n",
            "Epoch 46/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0157 - accuracy: 0.9943 - val_loss: 1.2820 - val_accuracy: 0.7725\n",
            "Epoch 47/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0154 - accuracy: 0.9946 - val_loss: 1.3310 - val_accuracy: 0.7900\n",
            "Epoch 48/100\n",
            "144/144 [==============================] - 102s 708ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 1.5876 - val_accuracy: 0.7775\n",
            "Epoch 49/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0225 - accuracy: 0.9919 - val_loss: 1.4521 - val_accuracy: 0.7713\n",
            "Epoch 50/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0204 - accuracy: 0.9936 - val_loss: 1.4428 - val_accuracy: 0.8025\n",
            "Epoch 51/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0262 - accuracy: 0.9904 - val_loss: 1.5474 - val_accuracy: 0.7725\n",
            "Epoch 52/100\n",
            "144/144 [==============================] - 102s 708ms/step - loss: 0.0181 - accuracy: 0.9937 - val_loss: 1.4780 - val_accuracy: 0.7887\n",
            "Epoch 53/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0187 - accuracy: 0.9936 - val_loss: 1.3077 - val_accuracy: 0.7800\n",
            "Epoch 54/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0154 - accuracy: 0.9942 - val_loss: 1.3576 - val_accuracy: 0.7812\n",
            "Epoch 55/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0177 - accuracy: 0.9936 - val_loss: 1.4858 - val_accuracy: 0.7850\n",
            "Epoch 56/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0265 - accuracy: 0.9918 - val_loss: 1.2339 - val_accuracy: 0.8100\n",
            "Epoch 57/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0174 - accuracy: 0.9933 - val_loss: 1.3544 - val_accuracy: 0.7837\n",
            "Epoch 58/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0217 - accuracy: 0.9919 - val_loss: 1.3672 - val_accuracy: 0.7763\n",
            "Epoch 59/100\n",
            "144/144 [==============================] - 102s 712ms/step - loss: 0.0184 - accuracy: 0.9932 - val_loss: 1.1891 - val_accuracy: 0.7950\n",
            "Epoch 60/100\n",
            "144/144 [==============================] - 102s 707ms/step - loss: 0.0117 - accuracy: 0.9967 - val_loss: 1.6600 - val_accuracy: 0.7812\n",
            "Epoch 61/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0318 - accuracy: 0.9890 - val_loss: 1.0825 - val_accuracy: 0.7763\n",
            "Epoch 62/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0150 - accuracy: 0.9944 - val_loss: 1.3417 - val_accuracy: 0.7900\n",
            "Epoch 63/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 1.4451 - val_accuracy: 0.7812\n",
            "Epoch 64/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 1.6095 - val_accuracy: 0.7825\n",
            "Epoch 65/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0150 - accuracy: 0.9950 - val_loss: 1.2286 - val_accuracy: 0.7937\n",
            "Epoch 66/100\n",
            "144/144 [==============================] - 102s 708ms/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 1.6661 - val_accuracy: 0.7738\n",
            "Epoch 67/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0128 - accuracy: 0.9951 - val_loss: 1.4423 - val_accuracy: 0.7713\n",
            "Epoch 68/100\n",
            "144/144 [==============================] - 102s 712ms/step - loss: 0.0190 - accuracy: 0.9928 - val_loss: 1.2647 - val_accuracy: 0.7887\n",
            "Epoch 69/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0189 - accuracy: 0.9931 - val_loss: 1.4789 - val_accuracy: 0.7650\n",
            "Epoch 70/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0170 - accuracy: 0.9943 - val_loss: 1.4546 - val_accuracy: 0.7962\n",
            "Epoch 71/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 1.5722 - val_accuracy: 0.7912\n",
            "Epoch 72/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0158 - accuracy: 0.9947 - val_loss: 1.2979 - val_accuracy: 0.7875\n",
            "Epoch 73/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0174 - accuracy: 0.9936 - val_loss: 1.1770 - val_accuracy: 0.7862\n",
            "Epoch 74/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0197 - accuracy: 0.9931 - val_loss: 1.5470 - val_accuracy: 0.7875\n",
            "Epoch 75/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0251 - accuracy: 0.9921 - val_loss: 1.2892 - val_accuracy: 0.8012\n",
            "Epoch 76/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0136 - accuracy: 0.9953 - val_loss: 1.2417 - val_accuracy: 0.8000\n",
            "Epoch 77/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0073 - accuracy: 0.9972 - val_loss: 1.4484 - val_accuracy: 0.7975\n",
            "Epoch 78/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0086 - accuracy: 0.9969 - val_loss: 1.6388 - val_accuracy: 0.8125\n",
            "Epoch 79/100\n",
            "144/144 [==============================] - 102s 707ms/step - loss: 0.0210 - accuracy: 0.9935 - val_loss: 1.4063 - val_accuracy: 0.7788\n",
            "Epoch 80/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0158 - accuracy: 0.9958 - val_loss: 1.3079 - val_accuracy: 0.7875\n",
            "Epoch 81/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0101 - accuracy: 0.9964 - val_loss: 1.5480 - val_accuracy: 0.7788\n",
            "Epoch 82/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0074 - accuracy: 0.9972 - val_loss: 1.4801 - val_accuracy: 0.7638\n",
            "Epoch 83/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0118 - accuracy: 0.9964 - val_loss: 1.4786 - val_accuracy: 0.7850\n",
            "Epoch 84/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0104 - accuracy: 0.9960 - val_loss: 1.5655 - val_accuracy: 0.7725\n",
            "Epoch 85/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0131 - accuracy: 0.9962 - val_loss: 1.4256 - val_accuracy: 0.7862\n",
            "Epoch 86/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0151 - accuracy: 0.9953 - val_loss: 1.3381 - val_accuracy: 0.7875\n",
            "Epoch 87/100\n",
            "144/144 [==============================] - 102s 711ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 1.4169 - val_accuracy: 0.7937\n",
            "Epoch 88/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0188 - accuracy: 0.9949 - val_loss: 1.2165 - val_accuracy: 0.8037\n",
            "Epoch 89/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0115 - accuracy: 0.9967 - val_loss: 1.5868 - val_accuracy: 0.7962\n",
            "Epoch 90/100\n",
            "144/144 [==============================] - 102s 708ms/step - loss: 0.0098 - accuracy: 0.9969 - val_loss: 1.6134 - val_accuracy: 0.7775\n",
            "Epoch 91/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0117 - accuracy: 0.9964 - val_loss: 1.4532 - val_accuracy: 0.7887\n",
            "Epoch 92/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0160 - accuracy: 0.9944 - val_loss: 1.2780 - val_accuracy: 0.7837\n",
            "Epoch 93/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0108 - accuracy: 0.9962 - val_loss: 1.4069 - val_accuracy: 0.7962\n",
            "Epoch 94/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 1.4228 - val_accuracy: 0.7962\n",
            "Epoch 95/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0100 - accuracy: 0.9964 - val_loss: 1.4403 - val_accuracy: 0.7875\n",
            "Epoch 96/100\n",
            "144/144 [==============================] - 102s 708ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 1.5194 - val_accuracy: 0.8000\n",
            "Epoch 97/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0132 - accuracy: 0.9965 - val_loss: 1.4974 - val_accuracy: 0.7850\n",
            "Epoch 98/100\n",
            "144/144 [==============================] - 102s 710ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 1.5959 - val_accuracy: 0.7725\n",
            "Epoch 99/100\n",
            "144/144 [==============================] - 102s 708ms/step - loss: 0.0105 - accuracy: 0.9961 - val_loss: 1.5433 - val_accuracy: 0.7850\n",
            "Epoch 100/100\n",
            "144/144 [==============================] - 102s 709ms/step - loss: 0.0148 - accuracy: 0.9949 - val_loss: 1.3447 - val_accuracy: 0.7900\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x=train_X_data,  # x : list\n",
        "                    y=train_Y_data,  # y: list\n",
        "                    batch_size= 50, # 128\n",
        "                    epochs=20,  # 100\n",
        "                    verbose=True,\n",
        "                    validation_data=(val_X_data, val_Y_data)  # tuple , 'NoneType' object is not callable\n",
        "                    #,callbacks=[early_stop]\n",
        "                    )\n",
        "\n",
        "\n",
        "# error\n",
        "# def _fixup_shape(images, labels):\n",
        "#    images.set_shape([None, 15, 256, 256, 3])\n",
        "#    labels.set_shape([None, 12])\n",
        "#    return images, labels\n",
        "# as_list() is not defined on an unknown tensorshape\n",
        "\n",
        "#model.load_weights(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I4PqBdE7qtY"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x=train_X_data,  # x : list\n",
        "                    y=train_Y_data,  # y: list\n",
        "                    batch_size=200,\n",
        "                    epochs=20, \n",
        "                    verbose=True,\n",
        "                    validation_data=(val_X_data, val_Y_data)  # tuple , 'NoneType' object is not callable\n",
        "                    #,callbacks=[early_stop]\n",
        "                    )\n",
        "\n",
        "\n",
        "# error\n",
        "# def _fixup_shape(images, labels):\n",
        "#    images.set_shape([None, 15, 256, 256, 3])\n",
        "#    labels.set_shape([None, 12])\n",
        "#    return images, labels\n",
        "# as_list() is not defined on an unknown tensorshape\n",
        "\n",
        "#model.load_weights(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHBdtsl5wisv"
      },
      "outputs": [],
      "source": [
        "fscore = model.evaluate(test_X_data, test_Y_data, verbose=0, batch_size=50)\n",
        "print(\"Test Accuracy of {}: {}\".format(model_name, score[1])) # Test Accuracy of HieAtt: 0.6171652674674988\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "word_rev_index = {}\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    word_rev_index[i] = word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CxEeMjwVMs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model- HAN (y:top50 code)"
      ],
      "metadata": {
        "id": "PU4R3ZeUwjXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1hyAuiuRwoLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6xeOnj4ixaza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sentence attention "
      ],
      "metadata": {
        "id": "Cn2OAfIGxbQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "owU6brmhxdOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "G86Do6r3xaLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-nC_EO_hyRQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxMiOzAkhyVE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFcw8ZTfhyYS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm9TvW4yztPq"
      },
      "source": [
        "# Visualization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7EUviwmhya6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo_1v22Mhydv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeVhR53QgEme"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "HAN-applied to mimic3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0slTUvaRdaAW7edqM/Am3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}